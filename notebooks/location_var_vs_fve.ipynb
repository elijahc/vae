{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import auc\n",
    "import argparse\n",
    "\n",
    "from src.trainer import Trainer\n",
    "from src.data_loader import Shifted_Data_Loader\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/elijahc/projects/vae/models/2019-01-14',\n",
       " '/home/elijahc/projects/vae/models/2019-01-15',\n",
       " '/home/elijahc/projects/vae/models/2019-01-16',\n",
       " '/home/elijahc/projects/vae/models/2019-01-17',\n",
       " '/home/elijahc/projects/vae/models/2019-01-18',\n",
       " '/home/elijahc/projects/vae/models/2019-01-19',\n",
       " '/home/elijahc/projects/vae/models/2019-01-20',\n",
       " '/home/elijahc/projects/vae/models/2019-01-21',\n",
       " '/home/elijahc/projects/vae/models/2019-01-22',\n",
       " '/home/elijahc/projects/vae/models/2019-01-23']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_root = '/home/elijahc/projects/vae'\n",
    "models_root = os.path.join(proj_root,'models')\n",
    "dates = ['2019-01-{}'.format(n) for n in np.arange(10)+14]\n",
    "paths = [os.path.join(models_root,d) for d in dates]\n",
    "trans_amt = np.arange(10)/10\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# from src.config import load_config\n",
    "\n",
    "def load_config(model_dir):\n",
    "    fp = os.path.join(model_dir,'params.json')\n",
    "    print('loading...',fp)\n",
    "    with open(fp, 'r') as f:\n",
    "        json_config = json.load(f)\n",
    "        config = argparse.Namespace()\n",
    "        for k in json_config.keys():\n",
    "            setattr(config, k, json_config[k])\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run():\n",
    "    def __init__(self,mapping=None):\n",
    "        if mapping is not None:\n",
    "            self.mapping = mapping\n",
    "        else:\n",
    "            self.mapping = {\n",
    "                'input':'input_image',\n",
    "                'y_class':'class',\n",
    "                'z_lat': 'z_lat',\n",
    "                'real': 'D',\n",
    "            }\n",
    "    \n",
    "    def from_keras_model(self,keras_model,mapping=None):\n",
    "        self.model = keras_model\n",
    "        if mapping is not None:\n",
    "            self.mapping = mapping\n",
    "        else:\n",
    "            mapping = self.mapping\n",
    "            \n",
    "        for attr_name,layer_name in mapping.items():\n",
    "            if attr_name is 'input':\n",
    "                setattr(self,attr_name,keras_model.input)\n",
    "            else:\n",
    "                setattr(self,attr_name,keras_model.get_layer(layer_name).output)\n",
    "\n",
    "        self.E = Model(\n",
    "            inputs=self.input,\n",
    "            outputs=[self.y_class,self.z_lat,self.real],\n",
    "            name='encoder'\n",
    "        )\n",
    "\n",
    "def run_from_dir(model_dir):\n",
    "    run = Run()\n",
    "    \n",
    "    config = load_config(model_dir)\n",
    "\n",
    "    # load json and create model\n",
    "    with open(os.path.join(config.model_dir,'model.json'), 'r') as json_file:\n",
    "        model_json = json_file.read()\n",
    "        model = model_from_json(model_json)\n",
    "\n",
    "        model.load_weights(os.path.join(config.model_dir,'weights.h5'))\n",
    "\n",
    "    run.from_keras_model(model)\n",
    "    run.config = config\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading... /home/elijahc/projects/vae/models/2019-01-14/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-15/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-16/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-17/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-18/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-19/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-20/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-21/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-22/params.json\n",
      "loading... /home/elijahc/projects/vae/models/2019-01-23/params.json\n"
     ]
    }
   ],
   "source": [
    "runs = [run_from_dir(mod_dir) for mod_dir in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fve_dx and fve_dy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fve_dx_norms = [np.nan_to_num(np.load(os.path.join(p,'fve_dx_norm.npy'))).mean(axis=0) for p in paths]\n",
    "fve_dy_norms = [np.nan_to_num(np.load(os.path.join(p,'fve_dy_norm.npy'))).mean(axis=0) for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_auc = [auc(np.arange(25)/25,f) for f in fve_dx_norms]\n",
    "dy_auc = [auc(np.arange(25)/25,f) for f in fve_dy_norms]\n",
    "\n",
    "dx_max = [f.max() for f in fve_dx_norms]\n",
    "dy_max = [f.max() for f in fve_dy_norms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make latent encoding generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        (None, 3136)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3000)         9411000     input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2000)         6002000     dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_merge (Dense)               (None, 35)           70035       dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               [(None, 10), (None,  0           enc_merge[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "class (Activation)              (None, 10)           0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_lat (Activation)              (None, 25)           0           lambda_4[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 35)           0           class[0][0]                      \n",
      "                                                                 z_lat[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "G (Model)                       (None, 3136)         127857      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "D (Dense)                       (None, 2)            4002        dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 15,614,894\n",
      "Trainable params: 15,614,574\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "r = runs[1]\n",
    "r.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  None\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 110us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.1\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 111us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.2\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 135us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.3\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 109us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.4\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 121us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.5\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 119us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.6\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 122us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.7\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 111us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.8\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 117us/step\n",
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.9\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n",
      "10000/10000 [==============================] - 1s 123us/step\n"
     ]
    }
   ],
   "source": [
    "classifiers = [Model(t.E.input,t.y_class) for t in runs]\n",
    "z_encoders = [Model(t.E.input,t.z_lat) for t in runs]\n",
    "dense_1_acts = [Model(t.E.input,t.model.layers[1].output) for t in runs]\n",
    "dense_2_acts = [Model(t.E.input,t.model.layers[2].output) for t in runs]\n",
    "\n",
    "classifier_acc = []\n",
    "z_encodings = []\n",
    "dense_1 = []\n",
    "dense_2 = []\n",
    "\n",
    "for r,classifier,z_encoder,translation,d1,d2 in zip(runs,classifiers,z_encoders,trans_amt,dense_1_acts,dense_2_acts):\n",
    "    config = r.config\n",
    "    if translation == 0:\n",
    "        tx=None\n",
    "    else:\n",
    "        tx = translation\n",
    "    \n",
    "    DL = Shifted_Data_Loader(dataset=config.dataset,flatten=True,\n",
    "                         rotation=None,\n",
    "                         translation=tx,\n",
    "                        )\n",
    "    r.DL = DL\n",
    "    \n",
    "    classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    _,acc = classifier.evaluate(DL.sx_test,DL.y_test_oh,batch_size=config.batch_size)\n",
    "    classifier_acc.append(acc)\n",
    "    \n",
    "    z_enc = z_encoder.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "    z_encodings.append(z_enc)\n",
    "    \n",
    "    d1_a = d1.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "    dense_1.append(d1_a)\n",
    "    \n",
    "    d2_a = d2.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "    dense_2.append(d2_a)\n",
    "#     y_lat = classifier.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elijahc/projects/vae/models/2019-01-17'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = runs[3]\n",
    "r.DL.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r,d1,d2,z,p in zip(runs,dense_1,dense_2,z_encodings,paths):\n",
    "#     os.makedirs(os.path.join(p,'layer_activations'))\n",
    "    np.save(os.path.join(p,'layer_activations','dense_1'),d1)\n",
    "    np.save(os.path.join(p,'layer_activations','dense_2'),d2)\n",
    "    np.save(os.path.join(p,'layer_activations','z_enc'),z)\n",
    "    np.save(os.path.join(p,'layer_activations','dx'),r.DL.dx[1].astype(int))\n",
    "    np.save(os.path.join(p,'layer_activations','dy'),r.DL.dy[1].astype(int))\n",
    "    np.save(os.path.join(p,'layer_activations','y_train'),r.DL.y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute variance explained of isomap embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elijahc/.pyenv/versions/jupyterlab-gpu/lib/python3.5/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import var_expl,norm_var_expl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "fa_10_iso_df = pd.read_pickle('../data/style_embeddings/fashion_mnist_isomap_10_neighbor.pk').set_index('test_idx').sort_index()\n",
    "\n",
    "isos = fa_10_iso_df.isomap_dim_1.values\n",
    "sub_dfs = []\n",
    "for cid in np.arange(10):\n",
    "    c_idxs = fa_10_iso_df.class_id.values==cid\n",
    "    subset_df = fa_10_iso_df[c_idxs]\n",
    "    scaler = MinMaxScaler(feature_range=(-14,14))\n",
    "    sc_isos = scaler.fit_transform(isos[c_idxs].reshape(-1,1)).flatten()\n",
    "    subset_df['scaled_isomap_dim_1'] = sc_isos\n",
    "    sub_dfs.append(subset_df)\n",
    "\n",
    "fa_10_iso_df = pd.concat(sub_dfs,axis=0).sort_index()\n",
    "# z_enc = z_encodings[3]\n",
    "# fve_iso = var_expl(features=z_enc,cond=dxs,bins=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>isomap_dim_1</th>\n",
       "      <th>scaled_isomap_dim_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2501.723983</td>\n",
       "      <td>7.665259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2509.234302</td>\n",
       "      <td>6.727526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>227.763048</td>\n",
       "      <td>4.840256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>764.240528</td>\n",
       "      <td>6.231729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-344.645871</td>\n",
       "      <td>-2.835388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class_id  isomap_dim_1  scaled_isomap_dim_1\n",
       "test_idx                                             \n",
       "0                9   2501.723983             7.665259\n",
       "1                2   2509.234302             6.727526\n",
       "2                1    227.763048             4.840256\n",
       "3                1    764.240528             6.231729\n",
       "4                6   -344.645871            -2.835388"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa_10_iso_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fve_isos_norm = []\n",
    "for z_enc in z_encodings:\n",
    "    isos = fa_10_iso_df.scaled_isomap_dim_1.values\n",
    "#     ve = np.nan_to_num(var_expl(features=z_enc,cond=isos,bins=21))\n",
    "    ve_norm = norm_var_expl(z_enc,isos,bins=21)\n",
    "    fve_isos_norm.append(ve_norm.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fve_dx_norms = []\n",
    "fve_dy_norms = []\n",
    "\n",
    "z_dfs = []\n",
    "for r,z_enc in zip(runs,z_encodings):\n",
    "    dxs = r.DL.dx[1]-14\n",
    "    dys = r.DL.dy[1]-14\n",
    "    \n",
    "    print(z_enc.shape)\n",
    "    fve_dx_norm = norm_var_expl(z_enc,dxs,bins=11).mean(axis=0)\n",
    "    fve_dy_norm = norm_var_expl(z_enc,dys,bins=11).mean(axis=0)\n",
    "    fve_dx_norms.append(fve_dx_norm)\n",
    "    fve_dy_norms.append(fve_dy_norm)\n",
    "    \n",
    "    df = pd.DataFrame.from_records({'dx':dxs,'dy':dys,'z_enc':z_enc.tolist()})\n",
    "    df = df.join(fa_10_iso_df,how='left')\n",
    "    z_dfs.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,tx in zip(z_dfs,trans_amt):\n",
    "    df.to_pickle('../data/style_embeddings/merge_{}.pk'.format(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_auc = [auc(np.arange(25)/25,f) for f in fve_dx_norms]\n",
    "dy_auc = [auc(np.arange(25)/25,f) for f in fve_dy_norms]\n",
    "iso_auc = [auc(np.arange(25)/25,f) for f in fve_isos_norm]\n",
    "\n",
    "dx_max = [f.max() for f in fve_dx_norms]\n",
    "dy_max = [f.max() for f in fve_dy_norms]\n",
    "isos_max = [f.max() for f in fve_isos_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trans_amt,classifier_acc)\n",
    "plt.ylim(0.5,1)\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/2019-01-28/acc_vs_spatial_variation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "fig,axs = plt.subplots(3,10,sharex=True,sharey=True,figsize=(18,5))\n",
    "# for ax in axs.flatten():\n",
    "#     ax.set_ylim(0,1)\n",
    "for fx,fy,fisos,i in zip(fve_dx_norms,fve_dy_norms,fve_isos_norm,np.arange(10)):\n",
    "    axs[0,i].scatter(np.arange(25),sorted(fx,reverse=True))\n",
    "    axs[1,i].scatter(np.arange(25),sorted(fy,reverse=True))\n",
    "    axs[2,i].scatter(np.arange(25),sorted(fisos,reverse=True))\n",
    "    \n",
    "\n",
    "    axs[0,0].set_ylabel('VE (dX)')\n",
    "    axs[1,0].set_ylabel('VE (dY)')\n",
    "    axs[2,0].set_ylabel('VE (style)')\n",
    "    \n",
    "    for ax in axs[2]:\n",
    "        ax.set_xticks([])\n",
    "plt.tight_layout()\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_ylim(0,1)\n",
    "plt.savefig('../figures/2019-01-28/unit_fve_waterfall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trans_amt,dx_auc)\n",
    "plt.plot(trans_amt,dy_auc)\n",
    "plt.plot(trans_amt,iso_auc)\n",
    "\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.legend(['dx','dy','style (Isomap)'])\n",
    "plt.tight_layout()\n",
    "fig_filename = 'auc_vs_spatial_variation.png'\n",
    "fig_fp = '../figures/2019-01-28/'+fig_filename\n",
    "plt.savefig(fig_fp)\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trans_amt,dx_max)\n",
    "plt.plot(trans_amt,dy_max)\n",
    "plt.plot(trans_amt,f_isos_max)\n",
    "plt.ylabel('fve_max')\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.legend(['dx','dy','style (Isomap)'])\n",
    "plt.tight_layout()\n",
    "fig_filename = 'fve_max_vs_spatial_variation.png'\n",
    "fig_fp = '../figures/2019-01-28/'+fig_filename\n",
    "plt.savefig(fig_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(trans_amt,y=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxs = runs[2].DL.dx[1]-14\n",
    "hist_edges = np.histogram_bin_edges(dxs,bins=14)\n",
    "bins,edges = np.histogram(dxs,bins=hist_edges,density=True)\n",
    "stats.entropy(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = stats.rv_histogram((bins,edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
