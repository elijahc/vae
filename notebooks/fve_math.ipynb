{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- When we navigate our visual environment we process high-dimensional visual information into low-D \"abstractions\"\n",
    "- A heavily studied example of this in theoretical neuroscience is object recognition\n",
    "    - High dimensional correlations in pixels are abstracted into low-D representation of category\n",
    "    - This has been extended into category-orthogonal information as well like \"style\" (Cheung et al)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating questions\n",
    "- The brain has finite \"space\" (in numbers of neurons) to represent these abstractions so how do we allocate them?\n",
    "- Does this allocation change if our environment changes? How so?\n",
    "- How do we learn to allocate representational space efficiently as a function of our inputs (e.g. visual invironment)?\n",
    "- How does a single network extract multi-faceted abstractions from a common input (e.g.  that extracts space, category, style from single input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an autoencoder that learns several orthogonal features\n",
    "- Use semi-supervised training by evaluating its reconstructions of the input\n",
    "- Analyze the represented space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We can generate an image dataset that contains varying amounts of spatial shifts (dx,dy) and if the network learns this property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to quantify how well the network does at abstracting a continuous environmental property\n",
    "    - For example: Object location (dx,dy) or style variations within an object\n",
    "\n",
    "- This relationship may not be (and probably isn't) linear so plain correlation may not work\n",
    "\n",
    "- One way to measure this is to discretize the range of a units activity and examine the property variance in that range\n",
    "\n",
    "- A \"well abstracted\" property should have a variance smaller than the properties global variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a feature vector $X_{n,t}$ that represents the activations of $n$ units in the latent space over $T$ trials\n",
    "\n",
    "- Define a contiguous property $P_t$ (e.g. dx from center of FOV) that is indexed by and varied across trials $t$\n",
    "\n",
    "- If the network learns to represent $P$ in $X_n$'s activity level a subset of activity level of $X_n$ should correspond with a subset $P$\n",
    "\n",
    "- Split the full activity range across all trials, $X_T$ into a discrete number of $b$ bins so $X_{n,b}$ is some mutually exclusive activity range and a subset of $X_{n,T}$\n",
    "\n",
    "- For each binned level of activity and calculate the variance of the property $\\sigma(P | X_b)$ or $\\sigma(P_b)$ for trials evoking activity $X_b$\n",
    "\n",
    "- A contiguous property $P$ that is \"well-represented\" by the neurons should have \"narrower\" variance band at each bin than the global variance of that properity\n",
    "\n",
    "- A poorly represented property would be expected to have binned variances, $\\sigma(P_{b})$, similar to global variance $\\sigma(P)$\n",
    "\n",
    "- $VE_R = E[\\frac{\\sigma(P)-\\sigma(P_b)}{\\sigma(P)}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze learning in environments with varying spatial variation \n",
    "  - [X] [Isomap style embedding](https://gist.github.com/elijahc/c7b2c8a9ef03148b3b4b8d2bac32c7c7#1-d-embedding)\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
