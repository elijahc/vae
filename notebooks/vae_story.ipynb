{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- When we navigate our visual environment we process high-dimensional visual information into low-D \"abstractions\" or representations\n",
    "- The primate visual cortex learns to extract this low-D information in a way that is highly concurrent, extracting several independent facets of a scene (object identity, position, pose, etc) simultaneously .\n",
    "\n",
    "- The visual cortex learns to extract this information predominantly unsupervised; with minimal or no labels or ground truth reference values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating questions\n",
    "- The brain has finite \"space\" (in numbers of neurons) to represent these abstractions so how do we allocate them?\n",
    "- Does this allocation change if our environment changes? How so?\n",
    "- How do we learn to allocate representational space efficiently as a function of our inputs (e.g. visual invironment)?\n",
    "- How does a single network extract multi-faceted abstractions from a common input (e.g.  that extracts space, category, style from single input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an autoencoder that learns several orthogonal features\n",
    "- Use semi-supervised training by evaluating its reconstructions of the input\n",
    "- Analyze the represented space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We can generate an image dataset that contains varying amounts of spatial shifts (dx,dy) and if the network learns this property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "#### Architecture\n",
    "We use an architecture depicted in the middle (S-AE)\n",
    "\n",
    "![mod](https://raw.githubusercontent.com/elijahc/tensorflow-generative-model-collections/master/assets/etc/S-AE_structures.png)\n",
    "\n",
    "- Z latent space has 25 units\n",
    "- I latent space has 10 for One-Hot encoding of class\n",
    "\n",
    "#### Loss\n",
    "The model is trained using a 3-part weighted loss function comprised of:\n",
    "- Categorical cross-entropy between Identity latent space $I$ and the true class $y$:\n",
    "    - $XEntropy\\big(y_i,I_i\\big)$\n",
    "    \n",
    "- Mean squared error between the input, $x$, and reconstruction $g(x)$:\n",
    "    - $MSE\\big(x_i,g(x_i)\\big)$\n",
    "    \n",
    "- Activation cross covariance between the latent spaces\n",
    "    - $XCov\\big(Z, I\\big)$\n",
    "    \n",
    "$$\n",
    "Loss = \\alpha \\cdot XEntropy\\big(y_i,I_i\\big) + \\beta \\cdot MSE\\big(x_i,g(x_i)\\big) + \\gamma \\cdot XCov\\big(Z, I\\big)\n",
    "$$\n",
    "\n",
    "#### Training\n",
    "\n",
    "- 5% of training data (n=60000) was withheld as a validation set for determining learning plateau\n",
    "- All models were trained until validation loss improvement on the last 5 trials fell below 0.05 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Embedding\n",
    "\n",
    "Cheung et al show that split autoencoder networks tend to represent \"style\" when forced to use Z for learning category-orthogonal info\n",
    "\n",
    "To measure the degree that Z represents style we would like a contiguous metric or property that represents \"style\".\n",
    "\n",
    "I used Isomap to learn a 1-D manifold embedding of all test set images (n=10000) which gives a surrogate contiguous \"style\" metric.\n",
    "\n",
    "Sorting images according to this 1D manifold embedding shows it does a decent job of grouping within category \"styles\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-neighbor Isomap\n",
    "<img src='https://raw.githubusercontent.com/elijahc/vae/master/figures/style_embeddings/isomap_5_neighbor_fashion.png' height ='250px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-neighbor Isomap\n",
    "![7_neighbor](https://raw.githubusercontent.com/elijahc/vae/master/figures/style_embeddings/isomap_7_neighbor_fashion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-neighbor Isomap\n",
    "![10_neighbor](https://raw.githubusercontent.com/elijahc/vae/master/figures/style_embeddings/isomap_10_neighbor_fashion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to quantify how well the network does at abstracting scene properties\n",
    "    \n",
    "> i.e. Object location (dx,dy) or style variations within an object category\n",
    "\n",
    "This relationship may not be (and probably isn't) linear so plain correlation may not work.\n",
    "One way to measure this is to discretize the range of a units activity and examine the property variance in that range.\n",
    "A \"well abstracted\" property should have a variance smaller than the properties global variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a feature vector $X_{n,t}$ that represents the activations of $n$ units in the latent space over $T$ trials\n",
    "\n",
    "- Define a contiguous property $P_t$ (e.g. dx from center of FOV) that is indexed by and varied across trials $t$\n",
    "\n",
    "- If the network learns to represent $P$ in $X_n$'s activity level a subset of activity level of $X_n$ should correspond with a subset $P$\n",
    "\n",
    "- Split the full activity range across all trials, $X_T$ into a discrete number of $b$ bins so $X_{n,b}$ is some mutually exclusive activity range and a subset of $X_{n,T}$\n",
    "\n",
    "- For each binned level of activity and calculate the variance of the property $\\sigma(P | X_b)$ or $\\sigma(P_b)$ for trials evoking activity $X_b$\n",
    "\n",
    "- A contiguous property $P$ that is \"well-represented\" by the neurons should have \"narrower\" variance band at each bin than the global variance of that properity\n",
    "\n",
    "- A poorly represented property would be expected to have binned variances, $\\sigma(P_{b})$, similar to global variance $\\sigma(P)$\n",
    "\n",
    "- $VE_R = E[\\frac{\\sigma(P)-\\sigma(P_b)}{\\sigma(P)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Model\n",
    "- All models retained high classification accuracy with only a slight decrease across increasing levels of spatial variation\n",
    "\n",
    "![pic](https://raw.githubusercontent.com/elijahc/vae/master/figures/2019-01-28/acc_vs_spatial_variation.png)\n",
    "\n",
    "\n",
    "### Latent Representation Z\n",
    "\n",
    "#### Representational Variance Explained\n",
    "![pic](https://raw.githubusercontent.com/elijahc/vae/master/figures/2019-01-28/unit_fve_waterfall.png)\n",
    "\n",
    "- Each plot of the grid shows the landscape of what the units in latent variable Z learned\n",
    "- Each column (1-10) is a different model trained on a dataset generated with increasing spatial variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic](https://raw.githubusercontent.com/elijahc/vae/master/figures/2019-01-28/auc_vs_spatial_variation.png)\n",
    "![pic](https://raw.githubusercontent.com/elijahc/vae/master/figures/2019-01-28/fve_max_vs_spatial_variation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shannon Mutual Information\n",
    "\n",
    "- I realized we could linearly rescale activity in Z arbitrarily from 0-N and bin them to ints\n",
    "- Do the same for dx, dy, and isomap embeddings.\n",
    "- Calculate joint probability distributions for each unit and dx over all trials (25 joint dists)\n",
    "- Use these to calculate shanon mutual mutual info for each unit\n",
    "\n",
    "![pic](https://raw.githubusercontent.com/elijahc/vae/master/figures/2019-01-28/unit_shanon_waterfall.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/elijahc/vae/master/figures/2019-01-28/shannon_auc_vs_spatial_variation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
