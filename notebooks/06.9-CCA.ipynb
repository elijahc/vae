{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import neptune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "from scipy.stats import ttest_ind as ttest,pearsonr\n",
    "import scipy\n",
    "import xarray as xr\n",
    "from scipy.spatial.distance import pdist,squareform,cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "# from tensorflow.python import keras as keras\n",
    "from keras.models import Model\n",
    "\n",
    "from src.results.experiments import _DateExperimentLoader\n",
    "from src.results.utils import raw_to_xr, dprime\n",
    "from src.results.neptune import get_model_files, load_models, load_assemblies, load_params, load_properties,prep_assemblies,NeptuneExperimentRun,generate_convnet_encoders\n",
    "from src.results.dicarlo import get_dicarlo_su, process_dicarlo,err_neuroids\n",
    "from src.data_loader import Shifted_Data_Loader\n",
    "from src.data_generator import ShiftedDataBatcher\n",
    "import src.rcca\n",
    "\n",
    "import brainscore\n",
    "from brainscore.assemblies import walk_coords,split_assembly\n",
    "from brainscore.assemblies import split_assembly\n",
    "# from brainscore.metrics import Score\n",
    "\n",
    "from brainio_base.assemblies import DataAssembly\n",
    "\n",
    "def set_style():\n",
    "    # This sets reasonable defaults for font size for\n",
    "    # a figure that will go in a paper\n",
    "    sns.set_context(\"talk\")\n",
    "    \n",
    "    # Set the font to be serif, rather than sans\n",
    "    sns.set(font='serif')\n",
    "    \n",
    "    # Make the background white, and specify the\n",
    "    # specific font family\n",
    "    sns.set_style(\"white\", {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Georgia\",\"Times New Roman\", \"Palatino\", \"serif\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEPTUNE_API_TOKEN']=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5tbCIsImFwaV9rZXkiOiI3ZWExMTlmYS02ZTE2LTQ4ZTktOGMxMi0wMDJiZTljOWYyNDUifQ==\"\n",
    "neptune.init('elijahc/DuplexAE')\n",
    "neptune.set_project('elijahc/DuplexAE')\n",
    "proj_root = '/home/elijahc/projects/vae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.results.neptune import load_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assembly_fn</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>batch_sz</th>\n",
       "      <th>bg</th>\n",
       "      <th>bg_contrast</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dir</th>\n",
       "      <th>encoder_arch</th>\n",
       "      <th>generator_arch</th>\n",
       "      <th>id</th>\n",
       "      <th>im_translation</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>recon_weight</th>\n",
       "      <th>su_selectivity_fn</th>\n",
       "      <th>xent_weight</th>\n",
       "      <th>y_dim</th>\n",
       "      <th>z_dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>512.0</td>\n",
       "      <td>natural</td>\n",
       "      <td>0.3</td>\n",
       "      <td>fashion_mnist</td>\n",
       "      <td>models/2019-11-04/DPX-29</td>\n",
       "      <td>convnet</td>\n",
       "      <td>resnet</td>\n",
       "      <td>DPX-29</td>\n",
       "      <td>0.75</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>512.0</td>\n",
       "      <td>natural</td>\n",
       "      <td>0.3</td>\n",
       "      <td>fashion_mnist</td>\n",
       "      <td>models/2019-11-04/DPX-30</td>\n",
       "      <td>convnet</td>\n",
       "      <td>resnet</td>\n",
       "      <td>DPX-30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset.nc</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>512.0</td>\n",
       "      <td>natural</td>\n",
       "      <td>0.3</td>\n",
       "      <td>fashion_mnist</td>\n",
       "      <td>models/2019-09-25/DPX-10</td>\n",
       "      <td>dense</td>\n",
       "      <td>resnet</td>\n",
       "      <td>DPX-10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>selectivity.pqt</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>512.0</td>\n",
       "      <td>natural</td>\n",
       "      <td>0.3</td>\n",
       "      <td>fashion_mnist</td>\n",
       "      <td>models/2019-09-25/DPX-16</td>\n",
       "      <td>dense</td>\n",
       "      <td>resnet</td>\n",
       "      <td>DPX-16</td>\n",
       "      <td>0.75</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  assembly_fn augmentation  batch_sz       bg  bg_contrast        dataset  \\\n",
       "0         NaN      dynamic     512.0  natural          0.3  fashion_mnist   \n",
       "1         NaN      dynamic     512.0  natural          0.3  fashion_mnist   \n",
       "2  dataset.nc      dynamic     512.0  natural          0.3  fashion_mnist   \n",
       "3         NaN      dynamic     512.0  natural          0.3  fashion_mnist   \n",
       "\n",
       "                        dir encoder_arch generator_arch      id  \\\n",
       "0  models/2019-11-04/DPX-29      convnet         resnet  DPX-29   \n",
       "1  models/2019-11-04/DPX-30      convnet         resnet  DPX-30   \n",
       "2  models/2019-09-25/DPX-10        dense         resnet  DPX-10   \n",
       "3  models/2019-09-25/DPX-16        dense         resnet  DPX-16   \n",
       "\n",
       "   im_translation  n_epochs  recon_weight su_selectivity_fn  xent_weight  \\\n",
       "0            0.75   54000.0           0.0               NaN         15.0   \n",
       "1            0.75   54000.0           1.0               NaN         15.0   \n",
       "2            0.75   54000.0           1.0   selectivity.pqt         15.0   \n",
       "3            0.75   54000.0           0.0               NaN         15.0   \n",
       "\n",
       "  y_dim z_dim  \n",
       "0    35    35  \n",
       "1    35    35  \n",
       "2    35    35  \n",
       "3    35    35  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_eids = [\n",
    "    'DPX-29',\n",
    "    'DPX-30',\n",
    "]\n",
    "dense_eids = [\n",
    "    'DPX-10',\n",
    "    'DPX-16',\n",
    "#     'DPX-27',\n",
    "]\n",
    "# eids = conv_eids+dense_eids\n",
    "conv_exps = neptune.project.get_experiments(id=conv_eids)\n",
    "dense_exps = neptune.project.get_experiments(id=dense_eids)\n",
    "exps = np.array(conv_exps+dense_exps)\n",
    "s_df = pd.DataFrame(list(load_configs(exps)))\n",
    "s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso,LassoCV,MultiTaskLassoCV,MultiTaskLasso,SGDRegressor\n",
    "from sklearn.svm import SVR,LinearSVR,NuSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.multioutput import MultiOutputRegressor,MultiOutputEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(x,neural_data,region=None,brain_region=['IT','V4'], cv=5, variation=[3],sortby='image_id',train_size=0.75):\n",
    "    var_lookup = stimulus_set[stimulus_set.variation.isin(variation)].image_id.values\n",
    "    x = x.where(x.image_id.isin(var_lookup),drop=True)\n",
    "    nd = neural_data.where(neural_data.image_id.isin(var_lookup),drop=True)\n",
    "    \n",
    "    num_images = x.shape[0]\n",
    "    out_recs = []\n",
    "#     {'region':[],\n",
    "#             'layer':[],\n",
    "#             'fve':[],\n",
    "#             'pearsonr':[],\n",
    "#             'p-value':[],\n",
    "#             'neuron':[],\n",
    "#             'depth':[],\n",
    "#             'iter':[],\n",
    "#            }\n",
    "    \n",
    "    cv_tr = []\n",
    "    cv_te = []\n",
    "    \n",
    "    for rand_delta in np.arange(cv):\n",
    "        tr_idx, te_idx, _,_ = train_test_split(np.arange(num_images),np.arange(num_images),train_size=train_size,random_state=np.random.randint(0,50)+rand_delta)\n",
    "        cv_tr.append(tr_idx)\n",
    "        cv_te.append(te_idx)\n",
    "    \n",
    "    tr,te, _,_ = train_test_split(np.arange(num_images),np.arange(num_images),train_size=train_size,random_state=7)\n",
    "\n",
    "    \n",
    "    for br in brain_region:\n",
    "        nd_reg = nd.sel(region=br)[:,:20]\n",
    "        if region is None:\n",
    "            region = np.unique(x.region.values)\n",
    "        \n",
    "        with tqdm(region,total=len(region)) as reg_iter:\n",
    "            for reg in reg_iter:\n",
    "                if reg == 'pixel':\n",
    "                    continue\n",
    "                else:\n",
    "                    x_reg = x.sel(region=reg)\n",
    "                    depth = np.unique(x_reg.layer.values)[0]\n",
    "                reg_iter.set_description('{}{} x {}{}'.format(reg,x_reg.shape,br,nd_reg.shape))\n",
    "                \n",
    "#             with tqdm(np.arange(nd_reg.shape[-1])) as neurons:\n",
    "#                 fve_mean = []\n",
    "#                 r_mean = []\n",
    "#                 neurons.set_description('{}{} x {}{}'.format(reg,x_reg.shape,br,nd_reg.shape))\n",
    "                \n",
    "#                 for n_idx in neurons:\n",
    "#             estimator = MultiOutputRegressor(SGDRegressor(early_stopping=True),n_jobs=5)\n",
    "                estimator=MultiOutputRegressor(LinearSVR(), n_jobs=5)\n",
    "    #                     estimator = SGDRegressor()\n",
    "    #                     estimator = SVR()\n",
    "                estimator.fit(x_reg.values[tr],nd_reg.values[tr])\n",
    "                y_pred = estimator.predict(x_reg.values[te])\n",
    "#                 print(type(y_pred))\n",
    "#                 print(type(nd_reg.values[te]),nd_reg.values[te].shape)\n",
    "    #             r,pv = pearsonr(nd_reg.values[te],y_pred)\n",
    "                fve = explained_variance_score(nd_reg.values[te],y_pred)\n",
    "\n",
    "    #                     score = estimator.score(x_reg.values,nd_reg.values[:,n_idx])\n",
    "\n",
    "    #                     r,pv = pearsonr(ab_vec[0],ab_vec[1])\n",
    "    #             r_mean.append(np.nan_to_num(r))\n",
    "    #             fve_mean.append(fve)\n",
    "                reg_iter.set_postfix(fve=fve)\n",
    "                out_recs.append({\n",
    "                    'region':br,\n",
    "                    'layer':reg,\n",
    "                    'fve':fve,\n",
    "    #                 'cc':r,\n",
    "    #                 'neuron':n_idx,\n",
    "                    'depth':depth,\n",
    "                })\n",
    "#                     out_dict['region'].append(br)\n",
    "#                     out_dict['layer'].append(reg)\n",
    "#                     out_dict['fve'].append(fve)\n",
    "#                     out_dict['pearsonr'].append(r)\n",
    "#                     out_dict['neuron'].append(n_idx)\n",
    "# #                     out_dict['iter'].append(n)\n",
    "#                     out_dict['depth'].append(depth)\n",
    "\n",
    "#                     neurons.set_postfix(r_max=np.max(r_mean), r_mean=np.mean(r_mean), r_var=np.var(r_mean), fve_max=np.max(fve_mean), fve_mean=np.mean(fve_mean))\n",
    "#     print({k:v.shape for k,v in out_dict.items()})\n",
    "\n",
    "    return pd.DataFrame.from_records(out_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cca(x,neural_data,region=None, brain_region=['IT','V4'], cv=5, n_components=5, variation=[3],sortby='image_id',train_size=0.75):\n",
    "    var_lookup = stimulus_set[stimulus_set.variation.isin(variation)].image_id.values\n",
    "    x = x.where(x.image_id.isin(var_lookup),drop=True)\n",
    "    nd = neural_data.where(neural_data.image_id.isin(var_lookup),drop=True)\n",
    "    \n",
    "    x = x.sortby(sortby)\n",
    "    nd = nd.sortby(sortby)\n",
    "    \n",
    "    assert list(getattr(x,sortby).values) == list(getattr(nd,sortby).values)\n",
    "    num_images = x.shape[0]\n",
    "    out_recs = []\n",
    "#     out_dict = {'region':[],\n",
    "#             'layer':[],\n",
    "#             'pearsonr':[],\n",
    "#             'fve':[],\n",
    "# #             'p-value':[],\n",
    "#             'iter':[],\n",
    "#             'depth':[],\n",
    "#            }\n",
    "    \n",
    "    cv_tr = []\n",
    "    cv_te = []\n",
    "    \n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=cv)\n",
    "    for tr,te in kf.split(np.arange(num_images)):\n",
    "        cv_tr.append(tr)\n",
    "        cv_te.append(te)\n",
    "    \n",
    "    for rand_delta in np.arange(cv):\n",
    "        tr_idx, te_idx, _,_ = train_test_split(np.arange(num_images),np.arange(num_images),train_size=train_size,random_state=np.random.randint(0,50)+rand_delta)\n",
    "        cv_tr.append(tr_idx)\n",
    "        cv_te.append(te_idx)\n",
    "    \n",
    "    for br in brain_region:\n",
    "        nd_reg = nd.sel(region=br)\n",
    "        \n",
    "        if region is None:\n",
    "            region = np.unique(x.region.values)\n",
    "            \n",
    "        for reg in region:\n",
    "            if reg == 'pixel':\n",
    "                continue\n",
    "            x_reg = x.sel(region=reg)\n",
    "            \n",
    "            depth = np.unique(x_reg.layer.values)[0]\n",
    "            with tqdm(zip(np.arange(cv),cv_tr,cv_te), total=cv) as t:\n",
    "                t.set_description('{}{} x {}{}'.format(reg,x_reg.shape,br,nd_reg.shape))\n",
    "                \n",
    "                r_mean = []\n",
    "                fve_mean = []\n",
    "                cca_mean = []\n",
    "                for n,tr,te in t:\n",
    "                    cca = CCA(n_components=n_components)\n",
    "                    cca.fit(x_reg.values[tr],nd_reg.values[tr])\n",
    "\n",
    "                    u,v = cca.transform(x_reg.values[te],nd_reg.values[te])\n",
    "                    \n",
    "                    y_pred = cca.predict(x_reg.values[te])\n",
    "                    y_true = nd_reg.values[te]\n",
    "                    \n",
    "                    fve = explained_variance_score(y_true,y_pred,multioutput='uniform_average')\n",
    "                    r_vals = [pearsonr(y_pred[:,i],y_true[:,i]) for i in range(y_pred.shape[-1])]\n",
    "                    \n",
    "                    cca_r = np.mean([pearsonr(u[:,i],v[:,i]) for i in np.arange(n_components)])\n",
    "\n",
    "#                     r_vals = [pearsonr(ab_vec[0][:,i],ab_vec[1][:,i]) for i in range(ab_vec[0].shape[-1])]\n",
    "                    \n",
    "                    r_mean.append(np.mean([r for r,v in r_vals]))\n",
    "                    cca_mean.append(cca_r)\n",
    "                    fve_mean.append(fve)\n",
    "                \n",
    "                    out_recs.append({\n",
    "                        'region':br,\n",
    "                        'layer':reg,\n",
    "                        'pearsonr': np.mean([r for r,v in r_vals]),\n",
    "                        'cca_r':cca_r,\n",
    "                        'fve':fve,\n",
    "                        'iter':n,\n",
    "                        'depth':depth,\n",
    "                    })\n",
    "                    \n",
    "                    t.set_postfix(pearson=np.mean(r_mean), cca=np.mean(cca_mean), fve=np.mean(fve_mean))\n",
    "                    \n",
    "    return pd.DataFrame.from_records(out_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elijahc/.pyenv/versions/fastai/lib/python3.6/site-packages/brainio_base/assemblies.py:213: FutureWarning: The inplace argument has been deprecated and will be removed in a future version of xarray.\n",
      "  xr_data.set_index(append=True, inplace=True, **coords_d)\n",
      "/home/elijahc/.pyenv/versions/fastai/lib/python3.6/site-packages/brainio_base/assemblies.py:247: FutureWarning: The inplace argument has been deprecated and will be removed in a future version of xarray.\n",
      "  result.reset_index(self.multi_group_name, drop=True, inplace=True)\n",
      "/home/elijahc/.pyenv/versions/fastai/lib/python3.6/site-packages/brainio_base/assemblies.py:248: FutureWarning: The inplace argument has been deprecated and will be removed in a future version of xarray.\n",
      "  result.set_index(append=True, inplace=True, **{self.multi_group_name: self.group_coord_names})\n"
     ]
    }
   ],
   "source": [
    "neural_data = brainscore.get_assembly(name=\"dicarlo.Majaj2015\")\n",
    "neural_data.load()\n",
    "stimulus_set = neural_data.attrs['stimulus_set']\n",
    "# # stimulus_set.to_csv('../data/dicarlo_images/stimulus_set.csv',index=False)\n",
    "neural_data = process_dicarlo(neural_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_imgs = np.load('../data/dicarlo_images/sm_imgs_56x56.npy')\n",
    "\n",
    "ids3 = stimulus_set[stimulus_set.variation.values==3].image_id.values\n",
    "sm_ims = list(zip(ids3,sm_imgs[stimulus_set.variation.values==3]))\n",
    "\n",
    "Xm,Xs = (sm_imgs.mean(),sm_imgs.std())\n",
    "scaled_sm_imgs = np.clip((sm_imgs-Xm)/Xs,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = stimulus_set[['image_id','object_name','category_name','variation','dy_px','dx_px','rxy']].rename(columns={'dx_px':'dx','dy_px':'dy'})\n",
    "metadata = {k:list(v.values()) for k,v in metadata.to_dict().items()}\n",
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for exp,name in zip(reversed(exps),['no-recon','w/ recon','w/ recon','no-recon']):\n",
    "#     run = NeptuneExperimentRun(proj_root,neptune_exp=exp)\n",
    "#     xrs = run.gen_assembly(scaled_sm_imgs, n_units=180, **metadata)\n",
    "#     lasso_df = lasso(xrs,neural_data,region=None,variation=[0,3],cv=2)\n",
    "#     lasso_df['model']= name\n",
    "#     lasso_df['arch']=run.get_config()['encoder_arch']\n",
    "#     dfs.append(lasso_df)\n",
    "\n",
    "# lasso_3 = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model DPX-29(arch=convnet, recon=0.0)...\n",
      "WARNING:tensorflow:From /home/elijahc/.pyenv/versions/fastai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/elijahc/.pyenv/versions/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Compiling model\n",
      "generating convolutional activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA: conv_4(5760, 6272):   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(n_components=0.8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-22b1e1bd2c40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeptuneExperimentRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneptune_exp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mxr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpcas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca_assembly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_sm_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'svd_solver'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/vae/notebooks/src/results/neptune.py\u001b[0m in \u001b[0;36mpca_assembly\u001b[0;34m(self, test_data, n_units, n_components, metadata, pca_kws)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpca_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0menc_pca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0mpca_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/fastai/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \"\"\"\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/fastai/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/fastai/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;31m# flip eigenvectors' sign to enforce deterministic output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/fastai/lib/python3.6/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0;32m--> 129\u001b[0;31m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obj = ['no-recon','w/ recon','w/ recon','no-recon']\n",
    "att = []\n",
    "for exp,o in zip(exps,obj):\n",
    "    run = NeptuneExperimentRun(proj_root,neptune_exp=exp)\n",
    "    xr,pcas = run.pca_assembly(scaled_sm_imgs, n_units=None, n_components=0.8, metadata=metadata, pca_kws={'svd_solver':'full'})\n",
    "    att.append(pcas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dfs = []\n",
    "recs = []\n",
    "for mod,o in zip(att,obj):\n",
    "    for k,v in mod.items():\n",
    "        df = pd.DataFrame.from_records({'fve':v.explained_variance_,\n",
    "                                        'fve_ratio':v.explained_variance_ratio_,\n",
    "                                        'component':np.arange(len(v.explained_variance_))})\n",
    "        df['arch'] = k[:4]\n",
    "        df['depth'] = int(k[-1:])\n",
    "        df['layer']=k\n",
    "        df['objective']=o\n",
    "        pca_dfs.append(df)\n",
    "pca_80 = pd.concat(pca_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_80['cum_fve'] = pca_80.groupby(['arch','objective','layer'])['fve_ratio'].transform('cumsum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_80.to_pickle('../data/cca/pca_80fve.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.FacetGrid(data=pca_80,row='objective',col='arch',hue='depth',sharex='col',margin_titles=True,\n",
    "                  ylim=(0,1),\n",
    "                  height=4, palette='plasma',legend_out=True,\n",
    "                 )\n",
    "# plt.xscale('log')\n",
    "g.map(sns.lineplot,'component','cum_fve').add_legend()\n",
    "for a in g.axes.ravel():\n",
    "    pass\n",
    "#     a.set_xscale('log')\n",
    "# g.map(plt.hlines,y=0.8,xmin=0,xmax=600,colors='k',linestyle='dashed')\n",
    "\n",
    "# sns.lineplot(x='index'y='fve_ratio',hue='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pca_80 = pca_80.groupby(['arch','objective','layer'])['fve'].count().reset_index().rename(columns={'fve':'n_components'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=count_pca_80,col='arch',row='objective',sharex='col',sharey=False,margin_titles=True,\n",
    "#                   ylim=(0,1),\n",
    "                  height=4, palette='plasma',legend_out=True,\n",
    "                 )\n",
    "# plt.xscale('log')\n",
    "g.map(plt.bar,'layer','n_components')\n",
    "g.fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model DPX-29(arch=convnet, recon=0.0)...\n",
      "Compiling model\n",
      "generating convolutional activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA: conv_4(5760, 6272):   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(n_components=500)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA: conv_1(5760, 12544): 100%|██████████| 7/7 [00:18<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_4': PCA(copy=True, iterated_power='auto', n_components=500, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False), 'conv_3': PCA(copy=True, iterated_power='auto', n_components=500, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False), 'conv_2': PCA(copy=True, iterated_power='auto', n_components=500, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False), 'conv_1': PCA(copy=True, iterated_power='auto', n_components=500, random_state=None,\n",
      "    svd_solver='auto', tol=0.0, whiten=False)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-078adc7c0f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     cca_df = cca(xr,neural_data[:,~neural_data.neuroid_id.isin(err_neuroids)],\n\u001b[1;32m     11\u001b[0m                  \u001b[0mvariation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                  region=None,brain_region=['IT','V4'],sortby='image_id')\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcca_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objective'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcca_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_arch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8b1e690eec3c>\u001b[0m in \u001b[0;36mcca\u001b[0;34m(x, neural_data, region, brain_region, cv, n_components, variation, sortby, train_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcv_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcv_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "pca_comps = [500, 250, 100, 100]\n",
    "obj = ['no-recon','w/ recon','w/ recon','no-recon']\n",
    "\n",
    "for exp,n_c,obj in zip(exps,pca_comps, obj):\n",
    "    run = NeptuneExperimentRun(proj_root,neptune_exp=exp)\n",
    "    xr,pca_objs = run.pca_assembly(scaled_sm_imgs, n_units=None, n_components=n_c, metadata=metadata)\n",
    "    \n",
    "    cca_df = cca(xr,neural_data[:,~neural_data.neuroid_id.isin(err_neuroids)],\n",
    "                 variation=[0,3],cv=6, n_components=1,\n",
    "                 region=None,brain_region=['IT','V4'],sortby='image_id')\n",
    "    cca_df['objective']= obj\n",
    "    cca_df['arch']=run.get_config()['encoder_arch']\n",
    "    dfs.append(cca_df)\n",
    "\n",
    "pca_cca_nc_1 = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cca_nc_1.to_pickle('../data/cca/cca(1_component)_w_pca.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.FacetGrid(col='arch',row='model', hue='region', data=pca_cca_3,height=4,sharex=False,sharey='row',margin_titles=True)\n",
    "g.map(sns.boxenplot,'layer','pearsonr').add_legend()\n",
    "g.fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.FacetGrid(col='region',row='model', hue='arch', data=pca_cca_3,height=4,sharex=False,sharey='row',margin_titles=True)\n",
    "g.map(sns.boxenplot,'depth','fve').add_legend()\n",
    "g.fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.FacetGrid(col='arch',row='region',hue='model', data=pca_cca_3,height=5,sharex=False)\n",
    "g.map(sns.boxenplot,'layer','fve').add_legend()\n",
    "g.fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.FacetGrid(col='arch',row='region',hue='model', data=cca_3,height=5,sharex=False,palette='viridis')\n",
    "g.map(sns.boxenplot,'layer','pearsonr').add_legend()\n",
    "g.fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(x='depth',y='fve',hue='model',data=cca_3.query('arch == \"dense\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='layer',y='pearsonr',style='model',hue='region',\n",
    "#                  data=conv_cca.query('{} == \"{}\"'.format(split_on,col)),\n",
    "             data=cca_3.query('region == \"IT\"'),)\n",
    "    \n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_df_all = cca(xrs,neural_data,variation=[0,3,6],cv=35,region=['conv_1','conv_2','conv_3','conv_4','y_enc','z_enc'],sortby='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "cca_df_all['model']='no-recon'\n",
    "cca_df['model'] = 'no-recon'\n",
    "g = sns.FacetGrid(col='region',row='model',data=cca_df,height=5)\n",
    "g.map(sns.stripplot,'layer','pearsonr')\n",
    "g.fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='layer',y='pearsonr',style='model',hue='region',\n",
    "#                  data=conv_cca.query('{} == \"{}\"'.format(split_on,col)),\n",
    "             data=cca_df,)\n",
    "    \n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.rcca as rcca\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def dicarlo_cca(data,stimulus_set,region,variation=[3],cv=10):\n",
    "    \n",
    "    \n",
    "#     print(data.image_id.values)\n",
    "#     print(nd.image_id.values)\n",
    "    \n",
    "    print('same order? \\t',list(data.sortby('image_id').image_id.values) == list(nd.sortby('image_id').image_id.values))\n",
    "    \n",
    "    print('model.shape\\t',data.shape)\n",
    "    print('dicarlo.shape\\t',nd.shape)\n",
    "    out_dict = {'region':[],\n",
    "#                 'variation':[],\n",
    "#                 'rdm':[],\n",
    "                'layer':[],\n",
    "                'pearsonr':[],\n",
    "                'p-value':[],\n",
    "                'iter':[],\n",
    "               }\n",
    "    xrs = []\n",
    "    ab_vectors = []\n",
    "    ccas = []\n",
    "        \n",
    "    cv_tr = []\n",
    "    cv_te = []\n",
    "    \n",
    "    num_images = data.shape[0]\n",
    "    print(num_images)\n",
    "    \n",
    "    for rand_delta in np.arange(cv):\n",
    "        tr_idx, te_idx, _,_ = train_test_split(np.arange(num_images),np.arange(num_images),train_size=0.75,random_state=np.random.randint(0,50)+rand_delta)\n",
    "        cv_tr.append(tr_idx)\n",
    "        cv_te.append(te_idx)\n",
    "        \n",
    "    for reg in region:\n",
    "        sub_dat = data.sel(region=reg)\n",
    "#         print(sub_dat)\n",
    "        \n",
    "        for brain_region in ['V4','IT']:\n",
    "            \n",
    "            pairing = '{} x {}'.format(reg,brain_region)\n",
    "            for n, tr,te in tqdm(zip(np.arange(cv),cv_tr,cv_te),total=cv,desc=pairing):\n",
    "                cca = CCA(n_components=1)\n",
    "                cca.fit(sub_dat.values[tr],nd.sel(region=brain_region).values[tr])\n",
    "            \n",
    "                ab_vec = cca.transform(sub_dat.values[te],nd.sel(region=brain_region).values[te])\n",
    "        \n",
    "                r,pv = pearsonr(ab_vec[0],ab_vec[1])\n",
    "\n",
    "                out_dict['region'].append(brain_region)\n",
    "                out_dict['layer'].append(reg)\n",
    "                out_dict['pearsonr'].append(r[0])\n",
    "                out_dict['p-value'].append(pv[0])\n",
    "                out_dict['iter'].append(n)\n",
    "            \n",
    "#             print(out_dict)\n",
    "        \n",
    "#         ccas.append(cca)\n",
    "        \n",
    "#         cca_score = r\n",
    "        \n",
    "#         cca_score = cca.score(sub_dat.values,nd.sel(region='IT').values)\n",
    "        \n",
    "#         cca = CCA(kernelcca = False, reg = 0.001, numCC = 2)\n",
    "    \n",
    "#         X_tr, X_te, y_tr, y_te = train_test_split(np.arange(2560),np.arange(2560))\n",
    "        \n",
    "#         data_vecs = [sub_dat.values,sub_dat.values,nd.sel(region='IT').values,nd.sel(region='IT').values]\n",
    "        \n",
    "#         idxs = [X_tr, X_te, y_tr, y_te]\n",
    "        \n",
    "#         X_tr,X_te, y_tr, y_te = tuple([d[idx] for d,idx in zip(data_vecs,idxs)])\n",
    "        \n",
    "# #         ,nd.sel(region='IT').values\n",
    "        \n",
    "#         print(X_tr.shape,y_tr.shape)\n",
    "#         print(X_te.shape,y_te.shape)\n",
    "        \n",
    "#         cca.train([X_tr,y_tr])\n",
    "        \n",
    "#         cca_score = cca.validate([X_te,y_te])\n",
    "#         print([t.shape for t in cca_score])\n",
    "\n",
    "#         xrs.append(cca_score)\n",
    "        \n",
    "    return out_dict\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-fastai (Python3.6.1)",
   "language": "python",
   "name": "py3-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
