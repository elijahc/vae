{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial and Identity Tandem VAE\n",
    "Inspired by Olshausen and Cheung's work we try to segregate identity from spatial information in an unsupervised way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- ~~Implement cross-covariance penalty~~\n",
    "    - https://stackoverflow.com/questions/45874928/how-to-compute-covariance-in-tensorflow\n",
    "    - https://arxiv.org/abs/1412.6583\n",
    "    - https://en.wikipedia.org/wiki/Cross-covariance\n",
    "    - ~~Needs to be per-batch basis, use regularization?~~ Just operate on the layer, it has shape [batch,dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import keras as keras\n",
    "from keras.layers import Dense,Input,Lambda,Concatenate,Activation\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.losses import categorical_crossentropy,logcosh\n",
    "import keras.backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import BaseLogger,RemoteMonitor,TerminateOnNaN\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "from src.models import build_dense\n",
    "from src.utils import ElasticSearchMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test Fasion CIFAR10data from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# (x_train, y_train,w_train), (x_test,y_test,w_test) = emnist.load_byclass()\n",
    "(x_train, y_train), (x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "class_ids = np.unique(y_train)\n",
    "masks_train = [y_train==i for i in class_ids]\n",
    "masks_test = [y_test==i for i in class_ids]\n",
    "\n",
    "y_test_oh = to_categorical(y_test,num_classes=10)\n",
    "y_train_oh = to_categorical(y_train,num_classes=10)\n",
    "\n",
    "digit_mask = lambda y: y<10 \n",
    "uppercase = lambda y: (y>=10) & (y<36)\n",
    "lowercase = lambda y: (y>=36) & (y<62)\n",
    "\n",
    "input_shape=(784,)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "# plt.imshow(x_train[masks[4]][10].reshape(28,28).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3072 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7d944a8c5d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'making training data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mletter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mnew_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "# Make dataset thats just a copy with random offsets\n",
    "num_train = len(y_train)\n",
    "num_test = len(y_test)\n",
    "\n",
    "# pre-allocate shifted inputs\n",
    "sx_train = np.empty((num_train,784*4))\n",
    "sx_test = np.empty((num_test,784*4))\n",
    "\n",
    "# pre-allocate list of dx,dy shifts for each image\n",
    "delta_train = np.empty((num_train,2))\n",
    "delta_test = np.empty((num_test,2))\n",
    "\n",
    "def random_offset(X,scale=2):\n",
    "    bg_size=(28*scale,28*scale)\n",
    "    \n",
    "    dx = int(np.random.randint(-7,7))+14\n",
    "    dy = int(np.random.randint(-7,7))+14\n",
    "    \n",
    "    dx = max(dx,0)\n",
    "    dx = min(dx,bg_size[0]-28)\n",
    "    \n",
    "    dy = max(dy,0)\n",
    "    dy = min(dy,bg_size[0]-28)\n",
    "#     print(dx,dy)\n",
    "    new_im = np.zeros(bg_size)\n",
    "    new_im[dx:dx+28,dy:dy+28] = letter\n",
    "    \n",
    "    return new_im,np.array([dx,dy])\n",
    "\n",
    "print('making training data...')\n",
    "for i in tqdm(np.arange(num_train)):\n",
    "    letter = x_train[i].reshape(28,28)\n",
    "    new_im,offsets = random_offset(letter,scale=2)\n",
    "    sx_train[i] = new_im.reshape(1,4*784)\n",
    "    delta_train[i] = offsets\n",
    "\n",
    "print('making testing data...')\n",
    "for i in tqdm(np.arange(num_test)):\n",
    "    letter = x_test[i].reshape(28,28)\n",
    "    new_im,offsets = random_offset(letter,scale=2)\n",
    "    sx_test[i] = new_im.reshape(1,4*784)\n",
    "    delta_test[i] = offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(delta_test[:,0],delta_test[:,1],bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a shifted digit on a larger background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 250\n",
    "print(y_train[masks_train[3]][i])\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
    "axs[0].imshow(x_train[masks_train[3]][i].reshape(28,28))\n",
    "axs[1].imshow(sx_train[masks_train[3]][i].reshape(28*2,28*2))\n",
    "\n",
    "axs[0].get_xaxis().set_visible(False)\n",
    "axs[0].get_yaxis().set_visible(False)\n",
    "axs[1].get_xaxis().set_visible(False)\n",
    "axs[1].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.savefig('./shifted_mnist_3.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "```\n",
    "|     Inputs (3136)     |\n",
    " \\      h1 (1500)      /\n",
    "  |     h2 (1500)     |\n",
    "  \n",
    "     |z_hat| |y_hat|\n",
    "     \n",
    "    /   h3 (1500)   \\\n",
    "   |    h4 (1500)    |\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dims = [3000,1500]\n",
    "z_dim = 2\n",
    "y_dim = 3\n",
    "\n",
    "# randomly shifted image\n",
    "inputs = Input(shape=(784*4,))\n",
    "# encoded = Dense(encoding_dim,activation='relu')(inputs)\n",
    "encoded = build_dense(inputs,encoding_dims,activations='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dist_sample = sampler( args=(mean,std) )\n",
    ">\n",
    ">parameterizes a normal distribution from a mean and std and samples it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(args):\n",
    "    mean,log_stddev = args\n",
    "    std_norm = K.random_normal(shape=(K.shape(mean)[0],K.shape(mean)[1]),mean=0,stddev=1)\n",
    "    \n",
    "    return mean + K.exp(log_stddev) * std_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean = Dense(z_dim,name='z_mean')(encoded)\n",
    "z_log_sigma = Dense(z_dim)(encoded)\n",
    "# \"layerize\" z_hat random variable\n",
    "lat_vec = Lambda(sampler,name='z_sample')([z_mean,z_log_sigma])\n",
    "\n",
    "y_hat_mean = Dense(y_dim,name='y_mean')(encoded)\n",
    "y_hat_sigma = Dense(y_dim,name='y_sigma')(encoded)\n",
    "y_hat = Lambda(sampler, name='y_hat')([y_hat_mean,y_hat_sigma])\n",
    "# y_hat = Dense(2,name='y_hat')(encoded)\n",
    "\n",
    "# latent class repr\n",
    "# y_hat = Dense(2,activation='sigmoid')(encoded)\n",
    "y_int = Dense(encoding_dims[1],activation='relu')(y_hat)\n",
    "# y_int = Dense(250,activation='relu')(y_int)\n",
    "y_class = Dense(10,activation='softmax')(y_int)\n",
    "\n",
    "# Concatenate with One-hot identity vector\n",
    "combo_vec = Concatenate()([lat_vec,y_hat])\n",
    "\n",
    "# Expand back out input dimensions (batch_size x im_size)\n",
    "\n",
    "decoded_mean = build_dense(combo_vec,[encoding_dims[1],encoding_dims[0]]+[4*784],activations=['relu','relu','sigmoid'])\n",
    "# decoded_mean = build_dense(combo_vec,[encoding_dims[1],encoding_dims[0]]+[4*784],activations=['relu','relu','sigmoid'])\n",
    "tandem_vae = Model(inputs,decoded_mean)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tandem_vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Loss function\n",
    "\n",
    "- Reconstruction loss (sum of squared error)\n",
    "\n",
    "$ \\sum\\limits_{n} (X - \\bar{X})^2 $\n",
    "- Cross-covariance (XCov) of latent vars (z_hat, y_hat)\n",
    "- Classification loss (categorical crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.losses import *\n",
    "        \n",
    "def acc(y_true,y_pred):\n",
    "    return categorical_accuracy(y_true,y_class)\n",
    "\n",
    "def kl_loss_tot(y_true,y_pred):\n",
    "    return kl_loss_z(y_true,y_pred)\n",
    "\n",
    "def xentropy(y_true,y_pred):\n",
    "    return 3*categorical_crossentropy(y_true,y_class)\n",
    "\n",
    "def recon_mse(y_true,y_pred):\n",
    "    return K.mean(K.sum(K.square(y_pred-inputs),axis=-1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = ReconstructionLoss(inputs=inputs,outputs=decoded_mean,weight=3)\n",
    "xcov = XCov(y_hat_mean,z_mean,weight=100)\n",
    "kl_loss_z = KLDivergenceLoss(z_log_sigma,z_mean,weight=0.001,name='DKL_z')\n",
    "# kl_loss_y = KLDivergenceLoss(y_hat_sigma,y_hat_mean, weight=0.0001, name='DKL_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true,y_pred):\n",
    "    total_loss = 0\n",
    "    loss_fns = [\n",
    "        K.sum(recon_loss(y_true,y_pred)),\n",
    "        K.sum(xcov(y_true,y_pred)),\n",
    "        K.sum(3*categorical_crossentropy(y_true,y_class)),\n",
    "#         K.sum(kl_loss_z(y_true,y_pred))/128,\n",
    "#         K.sum(kl_loss_y(y_true,y_pred))\n",
    "    ]   \n",
    "    for L in loss_fns:\n",
    "        total_loss += L\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "tandem_vae.compile(loss=vae_loss,optimizer='adagrad',metrics=[recon_loss,recon_mse,xcov,acc,xentropy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = RemoteMonitor(root='http://localhost:9200',path='/tensorflow/train_batch/')\n",
    "examples=10\n",
    "choices = np.random.choice(np.arange(len(y_test)),examples)\n",
    "test_ims = sx_train[choices[:3]]\n",
    "# print(test_ims.shape)\n",
    "\n",
    "es_logger = ElasticSearchMonitor(root='http://localhost:9200',path='/tensorflow')\n",
    "ToN = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tandem_vae.fit(x=sx_train,y=y_train_oh,validation_split=0.1,\n",
    "               shuffle=True,\n",
    "               epochs=7,\n",
    "               batch_size=64,\n",
    "               callbacks=[es_logger,ToN],\n",
    "               verbose=0\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tandem_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_encoder = Model(inputs,z_mean)\n",
    "y_hat_encoder = Model(inputs,y_hat)\n",
    "y_int_encoder = Model(inputs,y_int)\n",
    "classifier = Model(inputs,y_class)\n",
    "decoder_inp = Input(shape=(y_dim+z_dim,))\n",
    "dec_layers = tandem_vae.layers[-3:]\n",
    "_gen_x = dec_layers[0](decoder_inp)\n",
    "_gen_x = dec_layers[1](_gen_x)\n",
    "_gen_x = dec_layers[2](_gen_x)\n",
    "generator = Model(decoder_inp,_gen_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_enc = z_mean_encoder.predict(sx_test,batch_size=128)\n",
    "y_class_enc = classifier.predict(sx_test,batch_size=128)\n",
    "y_hat_enc = y_hat_encoder.predict(sx_test,batch_size=128)\n",
    "y_int_enc = y_int_encoder.predict(sx_test,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n",
    "plt.scatter(y_hat_enc[:,1],y_hat_enc[:,2],c=y_test,cmap='magma')\n",
    "plt.xlabel('y_0')\n",
    "plt.ylabel('y_1')\n",
    "plt.title(r\"Latent Dimension $\\hat{Y}$\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tandem_vae.save_weights('../models/tandem_vae/vae_weights.h5')\n",
    "# generator.save_weights('../models/tandem_vae/generator_weights.h5')\n",
    "# lat_encoder.save_weights('../models/tandem_vae/encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy'])\n",
    "classifier_perf = classifier.evaluate(sx_test,to_categorical(y_test,num_classes=10))\n",
    "\n",
    "print('Classification Accuracy: ',classifier_perf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2)\n",
    "\n",
    "axs[0].hist2d(z_mean_enc[:,0],z_mean_enc[:,1]);\n",
    "axs[1].hist2d(delta_test[:,0],delta_test[:,1]);\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=5\n",
    "sns.set_context('talk')\n",
    "# sns.set_style('whitegrid')\n",
    "y_test_oh = to_categorical(y_test,num_classes=10)\n",
    "\n",
    "z0mean = z_mean_enc[:,0].mean()\n",
    "z1mean = z_mean_enc[:,1].mean()\n",
    "z0_sigma = z_mean_enc[:,0].std()\n",
    "z1_sigma = z_mean_enc[:,1].std()\n",
    "# z2_sigma = x_test_lat_enc[:,2].std()\n",
    "\n",
    "fig,axs = plt.subplots(examples,4,figsize=(6,8))\n",
    "choices = np.random.choice(np.arange(len(y_test)),examples)\n",
    "# lat_vec_ = z_mean_enc[choices]\n",
    "lat_vec_ = np.concatenate([z_mean_enc[choices],y_hat_enc[choices]],axis=1)\n",
    "print(lat_vec_.shape)\n",
    "dec_test = generator.predict(lat_vec_)\n",
    "\n",
    "# print(x_test_encoded[choices])\n",
    "\n",
    "for i,idx in enumerate(choices):\n",
    "    rec_true_im = x_test[idx].reshape(28,28)\n",
    "    in_im = sx_test[idx].reshape(28*2,28*2)\n",
    "    dec_im = dec_test[i].reshape(28*2,28*2)\n",
    "    \n",
    "    axs[i,0].imshow(rec_true_im)\n",
    "    axs[i,0].set_xticklabels([])\n",
    "    axs[i,0].set_yticklabels([])\n",
    "    \n",
    "    axs[i,1].imshow(in_im)\n",
    "    axs[i,1].set_xticklabels([])\n",
    "    axs[i,1].set_yticklabels([])\n",
    "    \n",
    "    axs[i,2].imshow(dec_im)\n",
    "    axs[i,2].set_xticklabels([])\n",
    "    axs[i,2].set_yticklabels([])\n",
    "#     axs[2,i].set_xlabel(\"class: {}\".format(str(np.argmax(y_class_enc[idx]))))\n",
    "    \n",
    "    axs[i,3].imshow(y_class_enc[idx].reshape(-1,1).T)\n",
    "    axs[i,3].set_xticklabels([])\n",
    "    axs[i,3].set_yticklabels([])\n",
    "    axs[i,3].set_xlabel(\"class: {}\".format(str(np.argmax(y_class_enc[idx]))))\n",
    "    \n",
    "# plt.tight_layout()\n",
    "# sns.despine(fig=fig)\n",
    "# plt.imshow(dec_test[2].reshape(28,28).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=2\n",
    "bins = 11\n",
    "\n",
    "all_sweeps = np.empty((examples,bins,4*784))\n",
    "\n",
    "\n",
    "z0s = np.linspace(z0mean+(-2*z0_sigma),z0mean+(2*z0_sigma),num=bins)\n",
    "z1s = np.linspace(z1mean+(-2*z1_sigma),z1mean+(2*z0_sigma),num=bins)\n",
    "# z2s = np.linspace(-2*z2_sigma,2*z2_sigma,num=10)\n",
    "\n",
    "fig,axs = plt.subplots(examples,bins,figsize=(15,int(15*(examples/bins))))\n",
    "\n",
    "for j,vec in enumerate(lat_vec_):\n",
    "    lat_size = vec.shape[-1]\n",
    "    sweep = np.empty((bins,lat_size))\n",
    "    \n",
    "    for i,z in enumerate(z0s):\n",
    "        sweep[i] = vec\n",
    "        sweep[i,0] = z\n",
    "    \n",
    "    im_sweep = generator.predict(sweep)\n",
    "    all_sweeps[j]=im_sweep\n",
    "    \n",
    "for i in np.arange(examples):\n",
    "    for j in np.arange(bins):\n",
    "        axs[i,j].imshow(all_sweeps[i,j].reshape(56,56))\n",
    "        axs[i,j].set_xticklabels([])\n",
    "        axs[i,j].set_yticklabels([])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(examples,bins,figsize=(14,int(14*(examples/bins))))\n",
    "\n",
    "for j,vec in enumerate(lat_vec_):\n",
    "    lat_size = vec.shape[-1]\n",
    "    sweep = np.empty((bins,lat_size))\n",
    "    for i,z in enumerate(z1s):\n",
    "        sweep[i] = vec\n",
    "        sweep[i,1] = z\n",
    "    \n",
    "    im_sweep = generator.predict(sweep)\n",
    "    all_sweeps[j]=im_sweep\n",
    "    \n",
    "for i in np.arange(examples):\n",
    "    for j in np.arange(bins):\n",
    "        axs[i,j].imshow(all_sweeps[i,j].reshape(56,56))\n",
    "        axs[i,j].set_xticklabels([])\n",
    "        axs[i,j].set_yticklabels([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "dxs = delta_test[:,0]\n",
    "dys = delta_test[:,1]\n",
    "\n",
    "g = sns.jointplot(dxs-14,z_mean_enc[:,0])\n",
    "g.set_axis_labels(xlabel='dx',ylabel='lat_z0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(dys-14,z_mean_enc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_mean_enc[:,0],z_mean_enc[:,1],c=dxs-14)\n",
    "plt.colorbar()\n",
    "plt.title(r\"dx in $\\hat{Z}$\")\n",
    "plt.xlabel(r\"$\\hat{Z_0}$\")\n",
    "plt.ylabel(r\"$\\hat{Z_1}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_mean_enc[:,0],z_mean_enc[:,1],c=dys-14)\n",
    "plt.colorbar()\n",
    "plt.title(r\"dy in $\\hat{Z}$\")\n",
    "plt.xlabel(r\"$\\hat{Z_0}$\")\n",
    "plt.ylabel(r\"$\\hat{Z_1}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axs = plt.subplots(1,2,figsize=(12,5))\n",
    "plt.scatter(dxs-14,dys-14,c=z_mean_enc[:,0])\n",
    "# con = plt.contourf(dxs-14,dys-14,z_mean_enc[:,0])\n",
    "# ax[1].scatter(dxs-14,dys-14,c=z_mean_enc[:,1])\n",
    "# ax[0].set_xlabel('dx')\n",
    "# ax[1].set_ylabel('dy')\n",
    "plt.colorbar()\n",
    "# plt.title('Latent Variable by x-shift (dx)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "plt.scatter(dxs-14,dys-14,c=z_mean_enc[:,1],)\n",
    "plt.xlabel('z_0')\n",
    "plt.ylabel('z_1')\n",
    "plt.title('Latent Variable by y-shift (dy)')\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import Isomap\n",
    "\n",
    "# iso = Isomap(n_neighbors=20,n_components=1)\n",
    "# lat_enc_iso = iso.fit_transform(X=x_test_lat_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_enc_iso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.squeeze(lat_enc_iso),dxs-14,c=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.squeeze(lat_enc_iso),dys-14,c=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x_test_loc_enc[:, 0], x_test_loc_enc[:, 1],\n",
    "            c=y_test,alpha=0.5\n",
    "           )\n",
    "plt.colorbar()\n",
    "plt.scatter(x=x_test_loc_enc[choices][:,0],y=x_test_loc_enc[choices][:,1],marker='+',\n",
    "            s=20**2,color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bqplot.pyplot as bqplt\n",
    "from bqplot import Tooltip\n",
    "import pandas as pd\n",
    "\n",
    "recs = []\n",
    "for i,cid in enumerate(y_test):\n",
    "    recs.append(dict(\n",
    "        loc_z=x_test_loc_enc[i],\n",
    "        loc_z0=x_test_loc_enc[i][0],\n",
    "        loc_z1=x_test_loc_enc[i][1],\n",
    "        id_z=x_test_encoded[i],\n",
    "        id_z0=x_test_encoded[i][0],\n",
    "        id_z1=x_test_encoded[i][1],\n",
    "        class_id=cid,\n",
    "        dx=delta_test[i][0],\n",
    "        dy=delta_test[i][1]\n",
    "    ))\n",
    "enc_df = pd.DataFrame.from_records(recs)\n",
    "enc_df.head()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df.to_pickle('./tandem_encoder_df.pk')\n",
    "np.save('./sx_test.npy',sx_test)\n",
    "np.save('./dec_test.npy',dec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bqplt.figure(title='Autoencoder Latent Space')\n",
    "# def_tt = Tooltip(fields=['x', 'y'], formats=['', '.2f'])\n",
    "# bqplt.scatter(enc_df.loc_z0.values,enc_df.loc_z1.values)\n",
    "# # plt.colorbar()\n",
    "# bqplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
