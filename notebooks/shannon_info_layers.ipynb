{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import dit\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from dit import ScalarDistribution\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_loader import Shifted_Data_Loader\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL = Shifted_Data_Loader('fashion_mnist',rotation=None,translation=0.8,autoload=False,flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/elijahc/projects/vae/models/2019-01-14',\n",
       " '/home/elijahc/projects/vae/models/2019-01-15',\n",
       " '/home/elijahc/projects/vae/models/2019-01-16',\n",
       " '/home/elijahc/projects/vae/models/2019-01-17',\n",
       " '/home/elijahc/projects/vae/models/2019-01-18',\n",
       " '/home/elijahc/projects/vae/models/2019-01-19',\n",
       " '/home/elijahc/projects/vae/models/2019-01-20',\n",
       " '/home/elijahc/projects/vae/models/2019-01-21',\n",
       " '/home/elijahc/projects/vae/models/2019-01-22',\n",
       " '/home/elijahc/projects/vae/models/2019-01-23']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_root = '/home/elijahc/projects/vae'\n",
    "models_root = os.path.join(proj_root,'models')\n",
    "dates = ['2019-01-{}'.format(n) for n in np.arange(10)+14]\n",
    "paths = [os.path.join(models_root,d) for d in dates]\n",
    "trans_amt = np.arange(10)/10\n",
    "fa_10_iso_df = pd.read_pickle('../data/style_embeddings/fashion_mnist_isomap_10_neighbor.pk').set_index('test_idx').sort_index()\n",
    "isos = fa_10_iso_df.isomap_dim_1.values\n",
    "\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dit.shannon.shannon.entropy(dist, rvs=None, rv_mode=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dit.shannon.entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_encodings = np.array([np.load(os.path.join(p,'layer_activations','z_enc.npy')) for p in paths])\n",
    "dense_1 = np.array([np.load(os.path.join(p,'layer_activations','dense_1.npy')) for p in paths])\n",
    "dense_2 = np.array([np.load(os.path.join(p,'layer_activations','dense_2.npy')) for p in paths])\n",
    "\n",
    "dxs = np.array([np.load(os.path.join(p,'layer_activations','dx.npy')) for p in paths])-14\n",
    "dys = np.array([np.load(os.path.join(p,'layer_activations','dy.npy')) for p in paths])-14\n",
    "cids = np.array([np.load(os.path.join(p,'layer_activations','y_train.npy')) for p in paths])\n",
    "dfs = [pd.DataFrame.from_records({'dx':dxs[i],'dy':dys[i],'class_id':cids[i],'eccentricity':[tx]*10000}) for i,tx in enumerate(trans_amt) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLs = [Shifted_Data_Loader('fashion_mnist',flatten=False,autoload=False) for _ in np.arange(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# classifiers = [Model(m.input,m.get_layer('class').output) for m in models]\n",
    "# class_encodings = [c.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elijahc/.pyenv/versions/jupyterlab-gpu/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "sub_dfs = []\n",
    "for cid in np.arange(10):\n",
    "    c_idxs = fa_10_iso_df.class_id.values==cid\n",
    "    subset_df = fa_10_iso_df[c_idxs]\n",
    "    scaler = MinMaxScaler(feature_range=(-14,14))\n",
    "    sc_isos = scaler.fit_transform(isos[c_idxs].reshape(-1,1)).flatten()\n",
    "    subset_df['scaled_isomap_dim_1'] = sc_isos\n",
    "    sub_dfs.append(subset_df)\n",
    "\n",
    "fa_10_iso_df = pd.concat(sub_dfs,axis=0).sort_index()\n",
    "iso = np.array([fa_10_iso_df.isomap_dim_1.values.tolist() for _ in np.arange(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunResult():\n",
    "    def __init__(self, z_raw, dx,dy,iso,class_id,dense_1,dense_2):\n",
    "        self.z_raw = z_raw\n",
    "        self.z_dim = z_raw.shape[-1]\n",
    "        self.dx = dx\n",
    "        self.dy = dy\n",
    "        self.dense_1 = dense_1\n",
    "        self.dense_2 = dense_2\n",
    "        self.isomap_1D_raw = iso\n",
    "        self.class_id = class_id\n",
    "        \n",
    "    def z_enc(self,feat_range=30):\n",
    "        z_n = [self.z_raw[:,n] for n in np.arange(self.z_dim)]\n",
    "        return [MinMaxScaler(feature_range=(0,feat_range)).fit_transform(nvec.reshape(-1,1)).flatten().astype(int) for nvec in z_n]\n",
    "    \n",
    "    def q_dense_2(self,feat_range=30):\n",
    "        # Computes quantized activation levels of dense_1 across feat_range levels\n",
    "        d_n = [self.dense_1[:,n] for n in np.arange(self.dense_2.shape[-1])]\n",
    "        return [MinMaxScaler(feature_range=(0,feat_range)).fit_transform(nvec.reshape(-1,1)).flatten().astype(int) for nvec in d_n]\n",
    "    \n",
    "    def q_dense_1(self,feat_range=30):\n",
    "        # Computes quantized activation levels of dense_1 across feat_range levels\n",
    "        d_n = [self.dense_1[:,n] for n in np.arange(self.dense_1.shape[-1])]\n",
    "        return [MinMaxScaler(feature_range=(0,feat_range)).fit_transform(nvec.reshape(-1,1)).flatten().astype(int) for nvec in d_n]\n",
    "    \n",
    "    def iso(self,feat_range=30):\n",
    "        return MinMaxScaler(feature_range=(0,feat_range)).fit_transform(self.isomap_1D_raw.reshape(-1,1)).flatten().astype(int)\n",
    "    \n",
    "    def joint_dist(self,Y,X=None,n_cores=3,verbose=False):\n",
    "        if X is None:\n",
    "            if verbose:\n",
    "                print('No X given, using z_enc as X')\n",
    "            X = self.z_enc()\n",
    "        if verbose:   \n",
    "            print('spinning up {} cores...'.format(n_cores))\n",
    "        pool = multiprocessing.Pool(processes=n_cores)\n",
    "        \n",
    "        pairs = [list(zip(n,Y)) for n in X]\n",
    "        n_vec = pool.map(Counter,pairs)\n",
    "        n_pmf = [{k:v/float(sum(C.values())) for k,v in C.items()} for C in n_vec]\n",
    "        n_cdists = pool.map(dit.Distribution,n_pmf)\n",
    "        \n",
    "        pool.close()\n",
    "#         pool.join()\n",
    "    #     n_dists = [ScalarDistribution(d) for d in n_pmf]\n",
    "\n",
    "        return n_cdists\n",
    "    \n",
    "    def entropy(self,X):\n",
    "        jdists = self.z_enc_joint_dist(X)\n",
    "        \n",
    "        return [dit.shannon.entropy(d) for d in jdists]\n",
    "    \n",
    "    def mutual_info(self,Y,X=None,n_cores=3):\n",
    "        # Calculates I(z_enc; X)\n",
    "        if X is None:\n",
    "            X = self.z_enc()\n",
    "            \n",
    "        jdists = self.joint_dist(Y,X,n_cores=n_cores)\n",
    "        \n",
    "        return [dit.shannon.mutual_information(d,[0],[1]) for d in jdists]\n",
    "    \n",
    "    def prior_layer_info(self,X,lname):\n",
    "        {\n",
    "            'z':self.dense_2,\n",
    "            'dense_2': self.dense_1,\n",
    "        }\n",
    "    \n",
    "    def conditional_entropy(self,X):\n",
    "        jdists = self.z_enc_joint_dist(X)\n",
    "        \n",
    "        cond_H = [dit.shannon.mutual_information(d,[1],[0]) for d in jdists]\n",
    "        \n",
    "        return cond_H\n",
    "\n",
    "\n",
    "# n,dx = make_joint_dists(z_encodings[3],dxs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_result_sets = [RunResult(z_encodings[i],dxs[i],dys[i],iso[i],cids[i],dense_1[i],dense_2[i]) for i in np.arange(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = z_result_sets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qd2 = rr.q_dense_2(feat_range=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z_dx_I: 100%|██████████| 10/10 [00:38<00:00,  4.09s/it]\n",
      "z_dy_I: 100%|██████████| 10/10 [00:35<00:00,  3.85s/it]\n",
      "z_iso_I: 100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n",
      "z_class_I: 100%|██████████| 10/10 [00:36<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "z_dx_I = [rr.mutual_info(rr.dx) for rr in tqdm(z_result_sets,desc='z_dx_I')]\n",
    "z_dy_I = [rr.mutual_info(rr.dy) for rr in tqdm(z_result_sets,desc='z_dy_I')]\n",
    "z_iso_I = [rr.mutual_info(rr.iso(feat_range=30)) for rr in tqdm(z_result_sets,desc='z_iso_I')]\n",
    "z_class_I = [rr.mutual_info(rr.class_id) for rr in tqdm(z_result_sets,desc='z_class_I')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_I_df = [pd.DataFrame.from_records({'dx':x,'dy':y,'style':i,'class':c}) for x,y,i,c in zip(z_dx_I,z_dy_I,z_iso_I,z_class_I)]\n",
    "for df,tx in zip(z_I_df,trans_amt):\n",
    "    df['translation']=tx\n",
    "    df['xcov']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zI = pd.concat(z_I_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d2_dx_I:   0%|          | 0/10 [00:00<?, ?it/s]Process ForkPoolWorker-127:\n",
      "Process ForkPoolWorker-125:\n",
      "Process ForkPoolWorker-123:\n",
      "Process ForkPoolWorker-126:\n",
      "Process ForkPoolWorker-124:\n",
      "Process ForkPoolWorker-121:\n",
      "Process ForkPoolWorker-128:\n",
      "Process ForkPoolWorker-122:\n",
      "Exception in thread Thread-131:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elijahc/.pyenv/versions/3.5.2/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/elijahc/.pyenv/versions/3.5.2/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/elijahc/.pyenv/versions/3.5.2/lib/python3.5/multiprocessing/pool.py\", line 429, in _handle_results\n",
      "    task = get()\n",
      "  File \"/home/elijahc/.pyenv/versions/3.5.2/lib/python3.5/multiprocessing/connection.py\", line 251, in recv\n",
      "    return ForkingPickler.loads(buf.getbuffer())\n",
      "_pickle.UnpicklingError: invalid load key, '\u0001'.\n",
      "\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "# This takes a long time, already done, just load them\n",
    "\n",
    "# print('calculating mutual info for dx...')\n",
    "d2_dx_I = [rr.mutual_info(rr.dx,X=rr.q_dense_2(),n_cores=8) for rr in tqdm(z_result_sets,desc='d2_dx_I')]\n",
    "\n",
    "# print('calculating mutual info for dy...')\n",
    "# d2_dy_I = [rr.mutual_info(rr.dy) for rr in d2_result_sets]\n",
    "\n",
    "# print('calculating mutual info for style (Iso)...')\n",
    "# d2_iso_I = [rr.mutual_info(rr.iso(feat_range=30)) for rr in d2_result_sets]\n",
    "\n",
    "# print('calculating mutual info for class...')\n",
    "# d2_class_I = [rr.mutual_info(rr.class_id) for rr in d2_result_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2_I_df = [pd.DataFrame.from_records({'dx':x,'dy':y,'style':i,'class':c}) for x,y,i,c in zip(d2_dx_I,d2_dy_I,d2_iso_I,d2_class_I)]\n",
    "# for df,tx in zip(d2_I_df,trans_amt):\n",
    "#     df['translation']=tx\n",
    "#     df['xcov']=10\n",
    "\n",
    "# for p,df in zip(paths,d2_I_df):\n",
    "#     df.to_pickle(os.path.join(p,'d2_smi_df.pk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_I_df = [pd.read_pickle(os.path.join(p,'d2_smi_df.pk')) for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_I_df[3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(z_I_df).to_pickle('../data/style_embeddings/z_I.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2I = pd.concat(d2_I_df)\n",
    "d2I.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.PairGrid(d2I,hue='translation',vars=['class','dx','dy','style'],hue_order=[0.9,0.3,0.1],palette='GnBu_d',diag_sharey=False)\n",
    "\n",
    "# g.map_diag(sns.countplot,)\n",
    "g.map_offdiag(plt.scatter,s=15)\n",
    "# g.set(ylim=(0, 1),xlim=(0,1),)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "g = sns.PairGrid(zI,hue='translation',vars=['class','dx','dy','style'],hue_order=[0.9,0.3,0.1],palette='GnBu_d',diag_sharey=False)\n",
    "\n",
    "g.map_offdiag(plt.scatter,s=15)\n",
    "# g.set(ylim=(0, 1),xlim=(0,1),)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,10,figsize=(30,3),\n",
    "#                        sharey=True,sharex=True,\n",
    "                      )\n",
    "\n",
    "for df,ax in zip(d2_I_df,axs):\n",
    "    sns.distplot(df['dx'],ax=ax,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,10,figsize=(30,3),sharey=True,sharex=True)\n",
    "# ax.set_ylim(-0.1,0.8)\n",
    "# ax.set_xlim(-0.1,0.8)\n",
    "df = d2_I_df[6]\n",
    "points = []\n",
    "for df,i in zip(d2_I_df,np.arange(10)):\n",
    "    \n",
    "    pts_0 = axs[i].scatter(df['dx'],df['dy'],c=df['class'],cmap='viridis',s=3)\n",
    "#     pts_1 = axs[1,i].scatter(df['dx'],df['dy'],c=df['style'],cmap='viridis',s=3)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('class')\n",
    "plt.colorbar(pts_0)\n",
    "# sns.scatterplot(x='dx',y='dy',hue='class',data=z_I_df[5],palette='plasma',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,10,figsize=(30,3),sharey=True,sharex=True)\n",
    "# ax.set_ylim(-0.1,0.8)\n",
    "# ax.set_xlim(-0.1,0.8)\n",
    "df = d2_I_df[6]\n",
    "points = []\n",
    "for df,i in zip(d2_I_df,np.arange(10)):\n",
    "    \n",
    "    pts_0 = axs[i].scatter(df['dx'],df['dy'],c=df['style'],cmap='viridis',s=3)\n",
    "#     pts_1 = axs[1,i].scatter(df['dx'],df['dy'],c=df['style'],cmap='viridis',s=3)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('class')\n",
    "plt.colorbar(pts_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_I_df = [df['spatial_var']=tx for df,tx in zip(z_I_df,tx_vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = result_set[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_vals = trans_amt\n",
    "plt.plot(tx_vals,np.array(z_dx_I).mean(axis=1))\n",
    "plt.plot(tx_vals,np.array(z_dy_I).mean(axis=1))\n",
    "plt.plot(tx_vals,np.array(z_iso_I).mean(axis=1))\n",
    "plt.plot(tx_vals,np.array(z_class_I).mean(axis=1))\n",
    "plt.legend(['dx','dy','style','class'])\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.ylabel('Avg Mutual Info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "fig,axs = plt.subplots(4,10,sharex=True,sharey=True,figsize=(20,6))\n",
    "for fx,fy,fisos,fclass,i in zip(z_dx_I,z_dy_I,z_iso_I,z_class_I,np.arange(10)):\n",
    "    axs[0,i].scatter(np.arange(25),sorted(fx,reverse=True))\n",
    "    axs[1,i].scatter(np.arange(25),sorted(fy,reverse=True))\n",
    "    axs[2,i].scatter(np.arange(25),sorted(fisos,reverse=True))\n",
    "    axs[3,i].scatter(np.arange(25),sorted(fclass,reverse=True))\n",
    "\n",
    "    axs[0,0].set_ylabel('I(dX|Z)')\n",
    "    axs[1,0].set_ylabel('I(dY|Z)')\n",
    "    axs[2,0].set_ylabel('I(S|Z)')\n",
    "    axs[3,0].set_ylabel('I(C|Z)')\n",
    "    \n",
    "    for ax in axs[3]:\n",
    "        ax.set_xticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/2019-01-28/unit_shanon_waterfall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_dx_I[i]) for i in np.arange(10)])\n",
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_dy_I[i]) for i in np.arange(10)])\n",
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_iso_I[i]) for i in np.arange(10)])\n",
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_class_I[i]) for i in np.arange(10)])\n",
    "plt.legend(['dx','dy','style','class'])\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.ylabel('AUC ()')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/2019-01-28/shannon_auc_vs_spatial_variation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
