{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import dit\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from dit import ScalarDistribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from src.data_loader import Shifted_Data_Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.8\n",
      "rot_max:  None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n"
     ]
    }
   ],
   "source": [
    "DL = Shifted_Data_Loader('fashion_mnist',rotation=None,translation=0.8,autoload=False,flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/elijahc/projects/vae/models/2019-01-14',\n",
       " '/home/elijahc/projects/vae/models/2019-01-15',\n",
       " '/home/elijahc/projects/vae/models/2019-01-16',\n",
       " '/home/elijahc/projects/vae/models/2019-01-17',\n",
       " '/home/elijahc/projects/vae/models/2019-01-18',\n",
       " '/home/elijahc/projects/vae/models/2019-01-19',\n",
       " '/home/elijahc/projects/vae/models/2019-01-20',\n",
       " '/home/elijahc/projects/vae/models/2019-01-21',\n",
       " '/home/elijahc/projects/vae/models/2019-01-22',\n",
       " '/home/elijahc/projects/vae/models/2019-01-23']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_root = '/home/elijahc/projects/vae'\n",
    "models_root = os.path.join(proj_root,'models')\n",
    "dates = ['2019-01-{}'.format(n) for n in np.arange(10)+14]\n",
    "paths = [os.path.join(models_root,d) for d in dates]\n",
    "trans_amt = np.arange(10)/10\n",
    "fa_10_iso_df = pd.read_pickle('../data/style_embeddings/fashion_mnist_isomap_10_neighbor.pk').set_index('test_idx').sort_index()\n",
    "isos = fa_10_iso_df.isomap_dim_1.values\n",
    "\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_encodings = np.array([np.load(os.path.join(p,'layer_activations','z_enc.npy')) for p in paths])\n",
    "dense_1 = np.array([np.load(os.path.join(p,'layer_activations','dense_1.npy')) for p in paths])\n",
    "dense_2 = np.array([np.load(os.path.join(p,'layer_activations','dense_2.npy')) for p in paths])\n",
    "\n",
    "dxs = np.array([np.load(os.path.join(p,'layer_activations','dx.npy')) for p in paths])-14\n",
    "dys = np.array([np.load(os.path.join(p,'layer_activations','dy.npy')) for p in paths])-14\n",
    "cids = np.array([np.load(os.path.join(p,'layer_activations','y_train.npy')) for p in paths])\n",
    "dfs = [pd.DataFrame.from_records({'dx':dxs[i],'dy':dys[i],'class_id':cids[i],'eccentricity':[tx]*10000}) for i,tx in enumerate(trans_amt) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "models = []\n",
    "for mfp in tqdm(paths):\n",
    "    with open(os.path.join(mfp,'model.json'),'r') as json_file:\n",
    "        m = model_from_json(json_file.read())\n",
    "        \n",
    "    m.load_weights(os.path.join(mfp,'weights.h5'))\n",
    "    \n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n",
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.75\n",
      "rot_max:  0.75\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 56, 56, 1)\n"
     ]
    }
   ],
   "source": [
    "DLs = [Shifted_Data_Loader('fashion_mnist',flatten=False,autoload=False) for _ in np.arange(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 56, 56, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DLs[0].sx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DLs[0].regen_shift_image(DLs[0].x_test_orig,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (28,28) into shape (0,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-64e989cbcefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mDL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDLs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mDL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test_orig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrdxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrdys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/vae/notebooks/src/data_loader.py\u001b[0m in \u001b[0;36mfrom_deltas\u001b[0;34m(self, im_stack, output, dxs, dys, scale)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mletter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mnew_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregen_shift_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/vae/notebooks/src/data_loader.py\u001b[0m in \u001b[0;36mregen_shift_image\u001b[0;34m(self, X, dx, dy, scale)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mnew_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mnew_im\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my_sz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_im\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (28,28) into shape (0,28)"
     ]
    }
   ],
   "source": [
    "from src.data_loader import prepare_keras_dataset\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# (x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "# (x_train,y_train),(x_test,y_test) = prepare_keras_dataset(x_train,y_train,x_test,y_test)\n",
    "\n",
    "sx_train = []\n",
    "for i in np.arange(13):\n",
    "    rdxs = dxs[i]\n",
    "    rdys = dys[i]\n",
    "    DL = DLs[i]\n",
    "    \n",
    "    DL.from_deltas(DL.x_test_orig,DL.sx_test,dxs=rdxs,dys=rdys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLs[0].sx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "classifiers = [Model(m.input,m.get_layer('class').output) for m in models]\n",
    "class_encodings = [c.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dfs = []\n",
    "for cid in np.arange(10):\n",
    "    c_idxs = fa_10_iso_df.class_id.values==cid\n",
    "    subset_df = fa_10_iso_df[c_idxs]\n",
    "    scaler = MinMaxScaler(feature_range=(-14,14))\n",
    "    sc_isos = scaler.fit_transform(isos[c_idxs].reshape(-1,1)).flatten()\n",
    "    subset_df['scaled_isomap_dim_1'] = sc_isos\n",
    "    sub_dfs.append(subset_df)\n",
    "\n",
    "fa_10_iso_df = pd.concat(sub_dfs,axis=0).sort_index()\n",
    "iso = np.array([fa_10_iso_df.isomap_dim_1.values.tolist() for _ in np.arange(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunResult():\n",
    "    def __init__(self,z_encodings,dx,dy,iso,class_id):\n",
    "        self.z_encoding_raw = z_encodings\n",
    "        self.z_dim = z_encodings.shape[-1]\n",
    "        self.dx = dx\n",
    "        self.dy = dy\n",
    "        self.isomap_1D_raw = iso\n",
    "        self.class_id = class_id\n",
    "        \n",
    "    def z_enc(self,feat_range=50):\n",
    "        z_n = [self.z_encoding_raw[:,n] for n in np.arange(self.z_dim)]\n",
    "        return [MinMaxScaler(feature_range=(0,feat_range)).fit_transform(nvec.reshape(-1,1)).flatten().astype(int) for nvec in z_n]\n",
    "\n",
    "    def iso(self,feat_range=50):\n",
    "        return MinMaxScaler(feature_range=(0,feat_range)).fit_transform(self.isomap_1D_raw.reshape(-1,1)).flatten().astype(int)\n",
    "    \n",
    "    def z_enc_joint_dist(self,X):\n",
    "\n",
    "        n_vec = [Counter(zip(n,X)) for n in self.z_enc()]\n",
    "        n_pmf = [{k:v/float(sum(C.values())) for k,v in C.items()} for C in n_vec]\n",
    "        n_cdists = [dit.Distribution(d) for d in n_pmf]\n",
    "    #     n_dists = [ScalarDistribution(d) for d in n_pmf]\n",
    "\n",
    "        return n_cdists\n",
    "    \n",
    "    def entropy(self,X):\n",
    "        jdists = self.z_enc_joint_dist(X)\n",
    "        \n",
    "        return [dit.shannon.entropy(d) for d in jdists]\n",
    "    \n",
    "    def mutual_info(self,X):\n",
    "        jdists = self.z_enc_joint_dist(X)\n",
    "        \n",
    "        return [dit.shannon.mutual_information(d,[0],[1]) for d in jdists]\n",
    "        \n",
    "    def conditional_entropy(self,X):\n",
    "        jdists = self.z_enc_joint_dist(X)\n",
    "        \n",
    "        cond_H = [dit.shannon.mutual_information(d,[1],[0]) for d in jdists]\n",
    "        \n",
    "        return cond_H\n",
    "\n",
    "\n",
    "# n,dx = make_joint_dists(z_encodings[3],dxs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_set = [RunResult(z_encodings[i],dxs[i],dys[i],iso[i],cids[i]) for i in np.arange(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dx_I = [rr.mutual_info(rr.dx) for rr in result_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dy_I = [rr.mutual_info(rr.dy) for rr in result_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_iso_I = [rr.mutual_info(rr.iso(feat_range=50)) for rr in result_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_class_I = [rr.mutual_info(rr.class_id) for rr in result_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_I_df = [pd.DataFrame.from_records({'dx':x,'dy':y,'style':i,'class':c}) for x,y,i,c in zip(z_dx_I,z_dy_I,z_iso_I,z_class_I)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,tx in zip(z_I_df,trans_amt):\n",
    "    df['translation']=tx\n",
    "    df['xcov']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(z_I_df).to_pickle('../data/style_embeddings/z_I.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,10,figsize=(20,4),sharey=True,sharex=True)\n",
    "# ax.set_ylim(-0.1,0.8)\n",
    "# ax.set_xlim(-0.1,0.8)\n",
    "df = z_I_df[6]\n",
    "points = []\n",
    "for df,i in zip(z_I_df,np.arange(10)):\n",
    "    pts_0 = axs[0,i].scatter(df['dx'],df['dy'],c=df['class'],cmap='viridis')\n",
    "    pts_1 = axs[1,i].scatter(df['dx'],df['dy'],c=df['style'],cmap='viridis')\n",
    "    \n",
    "# plt.colorbar(points[0])\n",
    "# sns.scatterplot(x='dx',y='dy',hue='class',data=z_I_df[5],palette='plasma',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_I_df = [df['spatial_var']=tx for df,tx in zip(z_I_df,tx_vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = result_set[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_vals = trans_amt\n",
    "plt.plot(tx_vals,np.array(z_dx_I).mean(axis=1))\n",
    "plt.plot(tx_vals,np.array(z_dy_I).mean(axis=1))\n",
    "plt.plot(tx_vals,np.array(z_iso_I).mean(axis=1))\n",
    "plt.plot(tx_vals,np.array(z_class_I).mean(axis=1))\n",
    "plt.legend(['dx','dy','style','class'])\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.ylabel('Avg Mutual Info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "fig,axs = plt.subplots(4,10,sharex=True,sharey=True,figsize=(20,6))\n",
    "for fx,fy,fisos,fclass,i in zip(z_dx_I,z_dy_I,z_iso_I,z_class_I,np.arange(10)):\n",
    "    axs[0,i].scatter(np.arange(25),sorted(fx,reverse=True))\n",
    "    axs[1,i].scatter(np.arange(25),sorted(fy,reverse=True))\n",
    "    axs[2,i].scatter(np.arange(25),sorted(fisos,reverse=True))\n",
    "    axs[3,i].scatter(np.arange(25),sorted(fclass,reverse=True))\n",
    "\n",
    "    axs[0,0].set_ylabel('I(dX|Z)')\n",
    "    axs[1,0].set_ylabel('I(dY|Z)')\n",
    "    axs[2,0].set_ylabel('I(S|Z)')\n",
    "    axs[3,0].set_ylabel('I(C|Z)')\n",
    "    \n",
    "    for ax in axs[3]:\n",
    "        ax.set_xticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../figures/2019-01-28/unit_shanon_waterfall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_dx_I[i]) for i in np.arange(10)])\n",
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_dy_I[i]) for i in np.arange(10)])\n",
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_iso_I[i]) for i in np.arange(10)])\n",
    "plt.plot(tx_vals,[auc(np.arange(25)/25.0,z_class_I[i]) for i in np.arange(10)])\n",
    "plt.legend(['dx','dy','style','class'])\n",
    "plt.xlabel('Spatial Variation')\n",
    "plt.ylabel('AUC ()')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/2019-01-28/shannon_auc_vs_spatial_variation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
