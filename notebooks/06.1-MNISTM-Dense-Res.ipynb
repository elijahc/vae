{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from src.data_loader import Shifted_Data_Loader,upsample_dataset\n",
    "from src.plot import orig_vs_transformed as plot_ovt\n",
    "from src.plot import enc_dec_samples\n",
    "# from src.models import GResNet,EDense,EResNet,EConvNet\n",
    "from src.test_models.drduplex import DRDuplex\n",
    "from src.config import get_config\n",
    "from src.trainer import Trainer\n",
    "from src.utils import prepare_dirs_and_logger\n",
    "from keras.datasets import fashion_mnist,mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config,_ = get_config()\n",
    "# Boilerplate\n",
    "setattr(config, 'proj_root', '/home/elijahc/projects/vae')\n",
    "setattr(config, 'log_dir', '/home/elijahc/projects/vae/logs')\n",
    "setattr(config, 'dev_mode',True)\n",
    "setattr(config, 'seed', 7)\n",
    "setattr(config, 'project','vae')\n",
    "setattr(config, 'ecc_max',4.8/8.0)\n",
    "setattr(config, 'bg_noise',0.8)\n",
    "setattr(config, 'contrast_level',0.8)\n",
    "# setattr(config, 'rot_max',90.0/360.0)\n",
    "setattr(config, 'rot_max',0)\n",
    "\n",
    "# Training Params\n",
    "setattr(config, 'batch_size', 512)\n",
    "setattr(config, 'dataset', 'fashion_mnist')\n",
    "setattr(config, 'epochs',1000)\n",
    "setattr(config, 'monitor', None)\n",
    "# setattr(config, 'lr', 10)\n",
    "# setattr(config, 'min_delta', 0.25)\n",
    "# setattr(config, 'monitor', 'val_loss')\n",
    "setattr(config, 'optimizer', 'adam')\n",
    "setattr(config, 'label_corruption',0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture Params\n",
    "setattr(config, 'enc_blocks', [128,256,512])\n",
    "setattr(config, 'enc_arch', 'dense')\n",
    "setattr(config, 'dec_blocks', [4,2,1])\n",
    "setattr(config, 'z_dim', 35)\n",
    "setattr(config, 'y_dim', 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.ecc_max == 0.:\n",
    "    translation_amt = None\n",
    "else:\n",
    "    translation_amt = config.ecc_max\n",
    "\n",
    "if config.rot_max == 0.:\n",
    "    rot_max = None\n",
    "else:\n",
    "    rot_max = config.rot_max\n",
    "    \n",
    "if config.bg_noise == 0.:\n",
    "    bg_noise = None\n",
    "else:\n",
    "    bg_noise = config.bg_noise\n",
    "\n",
    "# Loss Weights\n",
    "setattr(config, 'xcov', 0)\n",
    "setattr(config, 'recon', 1)\n",
    "setattr(config, 'xent', 15)\n",
    "# setattr(config,'model_dir','/home/elijahc/projects/vae/models/2019-06-07/recon_{}_xent_{}/label_corruption_{}'.format(config.recon,config.xent,config.label_corruption))\n",
    "setattr(config,'model_dir','/home/elijahc/projects/vae/models/2019-06-05/xent_{}_recon_{}_{}/bg_noise_{}'.format(config.xent,config.recon,config.enc_arch,config.bg_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'bg_noise': 0.8,\n",
       " 'contrast_level': 0.8,\n",
       " 'dataset': 'fashion_mnist',\n",
       " 'dec_blocks': [4, 2, 1],\n",
       " 'dev_mode': True,\n",
       " 'ecc_max': 0.6,\n",
       " 'enc_arch': 'dense',\n",
       " 'enc_blocks': [128, 256, 512],\n",
       " 'enc_layers': [500, 500],\n",
       " 'epochs': 1000,\n",
       " 'label_corruption': 0.0,\n",
       " 'log_dir': '/home/elijahc/projects/vae/logs',\n",
       " 'log_level': 'INFO',\n",
       " 'model_dir': '/home/elijahc/projects/vae/models/2019-06-05/xent_15_recon_1_dense/bg_noise_0.8',\n",
       " 'monitor': None,\n",
       " 'optimizer': 'adam',\n",
       " 'proj_root': '/home/elijahc/projects/vae',\n",
       " 'project': 'vae',\n",
       " 'recon': 1,\n",
       " 'rot_max': 0,\n",
       " 'seed': 7,\n",
       " 'xcov': 0,\n",
       " 'xent': 15,\n",
       " 'y_dim': 35,\n",
       " 'z_dim': 35}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "if not config.dev_mode:\n",
    "    print('setting up...')\n",
    "    prepare_dirs_and_logger(config)\n",
    "    \n",
    "vars(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (56, 56, 1)\n",
      "dataset:  fashion_mnist\n",
      "background:  None\n",
      "blend mode:  None\n",
      "scale:  2\n",
      "tx_max:  0.6\n",
      "rot_max:  None\n",
      "contrast_level:  1\n",
      "noise_mode:  uniform\n",
      "  amount: 1\n",
      "  width: 0.8\n",
      "creating noise uniform({'amount': 1, 'width': 0.8})...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train images: 100%|██████████| 120000/120000 [00:05<00:00, 23824.17it/s]\n",
      "test_images: 100%|██████████| 10000/10000 [00:00<00:00, 30950.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding noise to training set\n"
     ]
    }
   ],
   "source": [
    "oversample_factor=2\n",
    "DL = Shifted_Data_Loader(dataset=config.dataset,flatten=False,num_train=60000*oversample_factor,\n",
    "                         translation=translation_amt,\n",
    "                         rotation=rot_max,\n",
    "#                          contrast_level=config.contrast_level,\n",
    "#                          bg='natural',\n",
    "#                          blend=None,\n",
    "                         noise_mode='uniform',\n",
    "                         noise_kws={\n",
    "                             'amount':1,\n",
    "                             'width':config.bg_noise,\n",
    "                         },\n",
    "                         bg_only=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt,idx = plot_ovt(DL,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(DL.fg_train[50].reshape(56,56),cmap='gray',vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL.sx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = DRDuplex(img_shape=(56,56,1),\n",
    "               num_classes=DL.num_classes,\n",
    "               recon=config.recon,\n",
    "               xent=config.xent,n_residual_blocks=4,\n",
    "#                kernel_regularization=1e-5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL.sx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pct = 0.05\n",
    "val_idxs = np.random.choice(np.arange(10000),int(val_pct*60000),replace=False)\n",
    "validation_set = (DL.sx_test[val_idxs],\n",
    "                  {'Classifier':DL.y_test_oh[val_idxs],\n",
    "                   'Generator':DL.fg_test[val_idxs]}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.train(config.epochs,DL,config.batch_size,verbose=0,shuffle=True,\n",
    "          validation_data=validation_set,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame.from_records(mod.combined.history.history)\n",
    "hist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "metrics = ['loss','Generator_loss','Classifier_acc']\n",
    "fig,axs = plt.subplots(nrows=len(metrics),sharex=True,figsize=(10,10))\n",
    "for metric_name,ax in zip(metrics,axs):\n",
    "    sns.scatterplot(data=hist_df[[metric_name,'val_'+metric_name]],ax=ax)\n",
    "#     ax.set_xscale('log')\n",
    "axs[2].hlines(y=(1.0/DL.num_classes),xmin=0,xmax=hist_df.index.values.max(),linestyles='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec(model,DL):\n",
    "    rand_im = np.random.randint(0,DL.x_train.shape[0])\n",
    "    im = DL.sx_train[rand_im]\n",
    "    y_true = DL.y_train_oh[rand_im]\n",
    "    \n",
    "    latent_rep = model.E.predict(im.reshape(1,56,56,1))\n",
    "    y_pred = model.Q.predict(im.reshape(1,56,56,1))\n",
    "\n",
    "    fig,axs = plt.subplots(2,2,figsize=(8,6))\n",
    "    \n",
    "    y_pred_axs = axs[1]\n",
    "    y_pred_axs[0].imshow(y_true.reshape(1,-1))\n",
    "    y_pred_axs[1].imshow(y_pred.reshape(1,-1))\n",
    "    im_axs = axs[0]\n",
    "    \n",
    "    im_axs[0].imshow(im.reshape(56,56),cmap='gray')\n",
    "    im_axs[0].set_title('Image; class: {}'.format(np.argmax(y_true)))\n",
    "    im_axs[1].set_title('Recon; class: {}'.format(np.argmax(y_pred)))\n",
    "    im_axs[1].imshow(model.G.predict(latent_rep).reshape(56,56),cmap='gray')\n",
    "    for ax in axs.ravel():\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec(mod,DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df['generalization_error'] = hist_df.val_loss - hist_df.loss\n",
    "hist_df['G_generalization_error'] = hist_df.val_Generator_loss - hist_df.Generator_loss\n",
    "hist_df['class_generalization_error'] = hist_df.val_Classifier_loss - hist_df.Classifier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=hist_df[['class_generalization_error']])\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def clean_config(config,keys=['dev_mode','log_dir','log_level','proj_root']):\n",
    "    c = vars(config)\n",
    "    for k in keys:\n",
    "        if k in c.keys():\n",
    "            del c[k]\n",
    "    \n",
    "    c['uploaded_by']='elijahc'\n",
    "    c['last_updated']= str(dt.datetime.now())\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_meta = clean_config(config)\n",
    "run_meta['project']='vae'\n",
    "# run_meta['ecc_max']=0.8\n",
    "run_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "run_conf = clean_config(config)\n",
    "\n",
    "with open(os.path.join(run_conf['model_dir'],'config.json'), 'w') as fp:\n",
    "    json.dump(run_conf, fp)\n",
    "\n",
    "hist_df.to_parquet(os.path.join(run_conf['model_dir'],'train_history.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = mod.G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_encoder = Model(mod.combined.input,mod.E.z_lat)\n",
    "y_encoder = Model(trainer.input,trainer.y_lat)\n",
    "classifier = Model(trainer.input,trainer.y_class)\n",
    "\n",
    "l3_encoder = Model(trainer.input,trainer.model.get_layer(name='dense_1').output)\n",
    "l1_encoder = Model(trainer.input,trainer.model.get_layer(name='conv2d_1').output)\n",
    "# l2_encoder = Model(trainer.input,trainer.model.get_layer(name='block_2_Add_2').output)\n",
    "# l2_encoder = Model(trainer.input,trainer.model.get_layer(name='block_4_Add_1').output)\n",
    "l2_encoder = Model(trainer.input,trainer.model.get_layer(name='conv2d_3').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_grad(model, inputs, outputs):\n",
    "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "res = classifier.evaluate(DL.sx_test,DL.y_test_oh,batch_size=config.batch_size)\n",
    "ts_error = 1-res[1]\n",
    "print(res[1])\n",
    "df = pd.DataFrame.from_records({'test_acc':[res[1]],\n",
    "                                'label_corruption':[config.label_corruption],\n",
    "                                'recon':[config.recon],\n",
    "                                'xent':[config.xent],\n",
    "                                'ecc_max':[config.ecc_max],\n",
    "                                'xcov': [config.xcov]})\n",
    "df.to_json(os.path.join(config.model_dir,'performance.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_s = l1_encoder.output_shape\n",
    "type(out_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_enc = l1_encoder.predict(DL.sx_test,batch_size=config.batch_size).reshape(10000,np.prod(l1_encoder.output_shape[1:]))\n",
    "l2_enc = l2_encoder.predict(DL.sx_test,batch_size=config.batch_size).reshape(10000,np.prod(l2_encoder.output_shape[1:]))\n",
    "l3_enc = l3_encoder.predict(DL.sx_test,batch_size=config.batch_size).reshape(10000,np.prod(l3_encoder.output_shape[1:]))\n",
    "\n",
    "z_enc = z_encoder.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "# y_lat = y_lat_encoder.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "y_enc = y_encoder.predict(DL.sx_test,batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import hashlib\n",
    "import random\n",
    "def raw_to_xr(encodings,l_2_depth,stimulus_set):\n",
    "    obj_names = [\n",
    "        \"T-shirt\",\n",
    "        \"Trouser\",\n",
    "        \"Pullover\",\n",
    "        \"Dress\",\n",
    "        \"Coat\",\n",
    "        \"Sandal\",\n",
    "        \"Dress Shirt\",\n",
    "        \"Sneaker\",\n",
    "        \"Bag\",\n",
    "        \"Ankle boot\",\n",
    "    ]\n",
    "    all_das = []\n",
    "    for layer,activations in encodings.items():\n",
    "        neuroid_n = activations.shape[1]\n",
    "        n_idx = pd.MultiIndex.from_arrays([\n",
    "            pd.Series(['{}_{}'.format(layer,i) for i in np.arange(neuroid_n)],name='neuroid_id'),\n",
    "            pd.Series([l_2_depth[layer]]*neuroid_n,name='layer'),\n",
    "            pd.Series([layer]*neuroid_n,name='region')\n",
    "        ])\n",
    "        p_idx = pd.MultiIndex.from_arrays([\n",
    "            stimulus_set.image_id,\n",
    "            stimulus_set.dx,\n",
    "            stimulus_set.dy,\n",
    "            stimulus_set.rxy,\n",
    "            stimulus_set.numeric_label.astype('int8'),\n",
    "            pd.Series([obj_names[i] for i in stimulus_set.numeric_label],name='object_name'),\n",
    "            pd.Series(stimulus_set.dx.values/28, name='tx'),\n",
    "            pd.Series(stimulus_set.dy.values/28, name='ty'),\n",
    "            pd.Series([1.0]*len(stimulus_set),name='s'),\n",
    "        ])\n",
    "        da = xarray.DataArray(activations.astype('float32'),\n",
    "                         coords={'presentation':p_idx,'neuroid':n_idx},\n",
    "                         dims=['presentation','neuroid'])\n",
    "        all_das.append(da)\n",
    "        \n",
    "    return xarray.concat(all_das,dim='neuroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {\n",
    "    'pixel':DL.sx_test.reshape(10000,np.prod(DL.sx_test.shape[1:])),\n",
    "    'dense_1':l1_enc,\n",
    "    'dense_2':l2_enc,\n",
    "    'dense_3':l3_enc,\n",
    "    'y_lat':y_enc,\n",
    "    'z_lat':z_enc\n",
    "}\n",
    "depths = {\n",
    "    'pixel':0,\n",
    "    'dense_1':1,\n",
    "    'dense_2':2,\n",
    "    'dense_3':3,\n",
    "    'y_lat':4,\n",
    "    'z_lat':4\n",
    "}\n",
    "slug = [(dx,dy,float(lab),float(random.randrange(20))) for dx,dy,rxy,lab in zip(DL.dx[1],DL.dy[1],DL.dtheta[1],DL.y_test)]\n",
    "image_id = [hashlib.md5(json.dumps(list(p),sort_keys=True).encode('utf-8')).digest().hex() for p in slug]\n",
    "stim_set = pd.DataFrame({'dx':DL.dx[1]-14,'dy':DL.dy[1]-14,'numeric_label':DL.y_test,'rxy':DL.dtheta[1],'image_id':image_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = raw_to_xr(encodings,depths,stim_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = raw_to_xr(encodings,depths,stim_set)\n",
    "from collections import OrderedDict\n",
    "def save_assembly(da,run_dir,fname,**kwargs):\n",
    "    da = da.reset_index(da.coords.dims)\n",
    "    da.attrs = OrderedDict()\n",
    "    with open(os.path.join(run_dir,fname), 'wb') as fp:\n",
    "        da.to_netcdf(fp,**kwargs)\n",
    "        \n",
    "    \n",
    "save_assembly(out,run_dir=config.model_dir,fname='dataset.nc',\n",
    "    format='NETCDF3_64BIT',\n",
    "#         engine=\n",
    "#         encoding=enc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_enc_tr = z_encoder.predict(DL.sx_train,batch_size=config.batch_size)\n",
    "# y_lat = y_lat_encoder.predict(DL.sx_test,batch_size=config.batch_size)\n",
    "# y_enc_tr = y_encoder.predict(DL.sx_train,batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(config.model_dir,'z_enc'),z_enc)\n",
    "np.save(os.path.join(config.model_dir,'l1_enc'),l1_enc)\n",
    "np.save(os.path.join(config.model_dir,'l2_enc'),l2_enc)\n",
    "np.save(os.path.join(config.model_dir,'y_enc'),y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lat_vec = np.concatenate([y_enc,z_enc],axis=1)\n",
    "_lat_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_enc_mu = np.mean(z_enc,axis=0)\n",
    "z_enc_cov = np.cov(z_enc,rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.multivariate_normal(z_enc_mu,z_enc_cov,size=50).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regen = generator.predict(_lat_vec,batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_im = np.random.randint(0,10000)\n",
    "plt.imshow(regen[rand_im].reshape(56,56),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lat_vec[rand_im]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_dec_samples(DL.x_train,DL.sx_train,z_enc_tr,y_enc_tr,generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_samples(DL.x_test,DL.sx_test,z_enc,y_enc,generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_enc2 = z_encoder.predict(DL2.sx_test,batch_size=config.batch_size)\n",
    "y_lat2 = y_encoder.predict(DL2.sx_test,batch_size=config.batch_size)\n",
    "_lat_vec2 = np.concatenate([y_lat2,z_enc2],axis=1)\n",
    "regen2 = generator.predict(_lat_vec2,batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plot import remove_axes,remove_labels\n",
    "from src.utils import gen_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = 5\n",
    "rand_im = np.random.randint(0,10000,size=examples)\n",
    "fix,axs = plt.subplots(examples,11,figsize=(8,4))\n",
    "_lat_s = []\n",
    "regen_s = []\n",
    "out = gen_trajectory(z_enc[rand_im],z_enc2[rand_im],delta=.25)\n",
    "out_y = gen_trajectory(y_enc[rand_im],y_lat2[rand_im],delta=.25)\n",
    "\n",
    "for z,y in zip(out,out_y):\n",
    "    _lat = np.concatenate([y,z],axis=1)\n",
    "    _lat_s.append(_lat)\n",
    "    regen_s.append(generator.predict(_lat,batch_size=config.batch_size))\n",
    "\n",
    "i=0\n",
    "for axr,idx in zip(axs,rand_im):\n",
    "    axr[0].imshow(DL.x_test[idx].reshape(28,28),cmap='gray')\n",
    "    axr[1].imshow(DL.sx_test[idx].reshape(56,56),cmap='gray')\n",
    "    axr[2].imshow(regen[idx].reshape(56,56),cmap='gray')\n",
    "    for j,a in enumerate(axr[3:-3]):\n",
    "        a.imshow(regen_s[j][i,:].reshape(56,56),cmap='gray')\n",
    "#         a.imshow(s.reshape(56,56),cmap='gray')\n",
    "    axr[-3].imshow(regen2[idx].reshape(56,56),cmap='gray')\n",
    "    axr[-2].imshow(DL2.sx_test[idx].reshape(56,56),cmap='gray')\n",
    "    axr[-1].imshow(DL2.x_test[idx].reshape(28,28),cmap='gray')\n",
    "    for a in axr:\n",
    "        remove_axes(a)\n",
    "        remove_labels(a)\n",
    "    i+=1\n",
    "# plt.imshow(regen[rand_im].reshape(56,56),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix.savefig('../../updates/2019-02-05/assets/img/translocate_{}.png'.format(translation_amt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdjsakl;fdsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import dit\n",
    "from dit import Distribution\n",
    "\n",
    "def mutual_information(X,Y):\n",
    "    XY_c = Counter(zip(X,Y))\n",
    "    XY_pmf = {k:v/float(sum(XY_c.values())) for k,v in XY_c.items()}\n",
    "    XY_jdist = Distribution(XY_pmf)\n",
    "        \n",
    "    return dit.shannon.mutual_information(XY_jdist,[0],[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
