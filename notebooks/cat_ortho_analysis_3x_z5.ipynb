{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from src.data_loader import Shifted_Data_Loader\n",
    "from src.metrics.dicarlo import r as dicarlo_r\n",
    "from src.metrics.dicarlo import dprime as dicarlo_dprime\n",
    "from src.metrics.dicarlo import selectivity as dicarlo_sel\n",
    "from src.results.experiments import *\n",
    "from src.results.file_loaders import *\n",
    "from src.results.processing import make_xr\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "    \n",
    "from collections import Counter\n",
    "import dit\n",
    "from dit import Distribution\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_loader(conf):\n",
    "    np.random.seed(7)\n",
    "    if conf['ecc_max'] == 0.0:\n",
    "        tx_max = None\n",
    "    else:\n",
    "        tx_max = conf['ecc_max']\n",
    "        \n",
    "    if 'rot_max' not in conf.keys() or conf['rot_max'] == 0.0:\n",
    "        rot_max = None\n",
    "    else:\n",
    "        rot_max = conf['rot_max']\n",
    "    DL = Shifted_Data_Loader(dataset=conf['dataset'],flatten=True,\n",
    "                             rotation=rot_max,\n",
    "                             translation=tx_max,num_train=60000,\n",
    "                            )\n",
    "    return DL\n",
    "\n",
    "def load_shifts(conf):\n",
    "    DL = load_data_loader(conf)\n",
    "    \n",
    "    return (DL.dx[1]-14,DL.dy[1]-14,DL.y_test)\n",
    "\n",
    "def load_train_history(run_dir,conf,filename='train_history.parquet'):\n",
    "    path = os.path.join(run_dir,filename)\n",
    "    dirname,fname = os.path.split(path)\n",
    "#     lab_corruption = np.round(float(dirname.split('/')[-1].split('_')[-1]),decimals=1)\n",
    "#     arch = dirname.split('/')[-2]\n",
    "    if conf is None:\n",
    "        return None\n",
    "    elif conf['recon'] == 0:\n",
    "        arch = 'no_recon'\n",
    "    else:\n",
    "        arch = 'recon'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        hist = pd.read_parquet(path)\n",
    "        hist['architecture'] = arch\n",
    "        hist['label_corruption'] = conf['label_corruption']\n",
    "        hist['ecc_max'] = conf['ecc_max']\n",
    "        hist['xent'] = conf['xent']\n",
    "        hist['recon'] = conf['recon']\n",
    "        hist['epoch'] = list(hist.index.values*3)\n",
    "#         hist['val_loss'] = sma(hist['val_loss'].values,win_size=3)\n",
    "#         hist['loss'] = sma(hist['loss'].values,win_size=3)\n",
    "        hist['val_dL'] = np.gradient(hist['val_loss'])\n",
    "        hist['test_err'] = 1-hist['val_class_acc']\n",
    "        hist['train_err'] = 1-hist['class_acc']\n",
    "        hist['recon_gen_err'] = hist.G_loss - hist.val_G_loss\n",
    "        hist['gen_err'] = hist.loss - hist.val_loss\n",
    "        hist['class_gen_err'] = hist.class_loss - hist.val_class_loss\n",
    "        hist['class_gen_acc'] = hist.class_acc - hist.val_class_acc\n",
    "\n",
    "        return hist\n",
    "\n",
    "\n",
    "def mutual_information(X,Y):\n",
    "    XY_c = Counter(zip(X,Y))\n",
    "    XY_pmf = {k:v/float(sum(XY_c.values())) for k,v in XY_c.items()}\n",
    "    XY_jdist = Distribution(XY_pmf)\n",
    "        \n",
    "    return dit.shannon.mutual_information(XY_jdist,[0],[1])\n",
    "\n",
    "def load_I(rd,fn,conf,feat_range=(0,30)):\n",
    "    \n",
    "    dxs,dys,y_test = load_shifts(conf)\n",
    "    z_enc = np.load(os.path.join(rd,fn))\n",
    "    z_dim = z_enc.shape[-1]\n",
    "    z_enc_scaled = [MinMaxScaler(feat_range).fit_transform(z_enc[:,i].reshape(-1,1)).tolist() for i in np.arange(z_dim)]\n",
    "    z_enc_scaled = np.squeeze(np.array(z_enc_scaled,dtype=int))\n",
    "    z_dx_I = [mutual_information(z_enc_scaled[i],dxs.astype(int)+14) for i in np.arange(z_dim)]\n",
    "    z_dy_I = [mutual_information(z_enc_scaled[i],dys.astype(int)+14) for i in np.arange(z_dim)]\n",
    "    z_class_I = [mutual_information(z_enc_scaled[i],y_test) for i in np.arange(z_dim)]\n",
    "    z_I_df = pd.DataFrame.from_records({'class':z_class_I,'dy':z_dy_I,'dx':z_dx_I})\n",
    "    z_I_df['class'] = z_I_df['class'].values.round(decimals=1)\n",
    "    z_I_df['ecc_max'] = conf['ecc_max']\n",
    "    z_I_df['recon'] = conf['recon']\n",
    "    z_I_df['xent'] = conf['xent']\n",
    "    z_I_df['label_corruption'] = conf['label_corruption']\n",
    "    \n",
    "    return z_I_df\n",
    "#     y_enc = np.load(os.path.join(rd,'y_enc.npy'))\n",
    "#     y_dim = y_enc.shape[-1]\n",
    "#     y_enc_scaled = [MinMaxScaler(feat_range).fit_transform(y_enc[:,i].reshape(-1,1)).tolist() for i in np.arange(y_dim)]\n",
    "#     y_enc_scaled = np.squeeze(np.array(y_enc_scaled,dtype=int))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_root = '/home/elijahc/projects/vae/models/2019-06-03'\n",
    "exp_root = '/home/elijahc/projects/vae/models/2019-05-24'\n",
    "runs = []\n",
    "for branches in os.listdir(exp_root):\n",
    "    for leaf in os.listdir(os.path.join(exp_root,branches)):\n",
    "        runs.append(os.path.join(exp_root,branches,leaf))\n",
    "\n",
    "runs = list(filter(lambda x: 'ipynb_checkpoints' not in x,runs))\n",
    "configs = [load_config(rd) for rd in runs]\n",
    "train_historys = [load_train_history(rd,conf) for rd,conf in zip(runs,configs)]\n",
    "perf = [load_performance(rd,conf,th) for rd,conf,th in zip(runs,configs,train_historys)]\n",
    "model_specs = [load_model_spec(rd) for rd in runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_xr(encodings,l_2_depth,stimulus_set):\n",
    "    obj_names = [\n",
    "        \"T-shirt\",\n",
    "        \"Trouser\",\n",
    "        \"Pullover\",\n",
    "        \"Dress\",\n",
    "        \"Coat\",\n",
    "        \"Sandal\",\n",
    "        \"Dress Shirt\",\n",
    "        \"Sneaker\",\n",
    "        \"Bag\",\n",
    "        \"Ankle boot\",\n",
    "    ]\n",
    "    all_das = []\n",
    "    for layer,activations in encodings.items():\n",
    "        neuroid_n = activations.shape[1]\n",
    "        n_idx = pd.MultiIndex.from_arrays([\n",
    "            pd.Series(['{}_{}'.format(layer,i) for i in np.arange(neuroid_n)],name='neuroid_id'),\n",
    "            pd.Series([l_2_depth[layer]]*neuroid_n,name='layer'),\n",
    "            pd.Series([layer]*neuroid_n,name='region')\n",
    "        ])\n",
    "        p_idx = pd.MultiIndex.from_arrays([\n",
    "            stimulus_set.image_id,\n",
    "            stimulus_set.dx,\n",
    "            stimulus_set.dy,\n",
    "            stimulus_set.numeric_label.astype('int8'),\n",
    "            pd.Series([obj_names[i] for i in stimulus_set.numeric_label],name='object_name'),\n",
    "            pd.Series(stimulus_set.dx.values/28, name='tx'),\n",
    "            pd.Series(stimulus_set.dy.values/28, name='ty'),\n",
    "            pd.Series([1.0]*len(stimulus_set),name='s'),\n",
    "        ])\n",
    "        da = xarray.DataArray(activations.astype('float32'),\n",
    "                         coords={'presentation':p_idx,'neuroid':n_idx},\n",
    "                         dims=['presentation','neuroid'])\n",
    "        all_das.append(da)\n",
    "        \n",
    "    return xarray.concat(all_das,dim='neuroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (3136,)\n",
      "dataset:  fashion_mnist\n",
      "scale:  2\n",
      "tx_max:  0.8\n",
      "rot_max:  None\n",
      "bg_noise: None\n",
      "loading fashion_mnist...\n",
      "sx_train:  (60000, 3136)\n",
      "making training data...\n",
      "making testing data...\n"
     ]
    }
   ],
   "source": [
    "DL = load_data_loader(configs[0])\n",
    "slug = [(dx,dy,float(lab),float(random.randrange(20))) for dx,dy,lab in zip(DL.dx[1],DL.dy[1],DL.y_test)]\n",
    "\n",
    "pixel_enc = {'pixel':DL.sx_test}\n",
    "\n",
    "\n",
    "encs = []\n",
    "stim_sets = []\n",
    "\n",
    "for mod in map(load_model,runs):\n",
    "    layer_encodings = []\n",
    "    layer_names = ['y_lat','z_lat','dense_1','dense_2','dense_3']\n",
    "\n",
    "    for l in layer_names:\n",
    "        encoder = Model(mod.input,mod.get_layer(l).output)\n",
    "        enc = encoder.predict(DL.sx_test)\n",
    "        layer_encodings.append(enc)\n",
    "    image_id = [hashlib.md5(json.dumps(list(p),sort_keys=True).encode('utf-8')).digest().hex() for p in slug]\n",
    "    stim_set = pd.DataFrame({'dx':DL.dx[1]-14,'dy':DL.dy[1]-14,'numeric_label':DL.y_test,'image_id':image_id})\n",
    "    enc = {k:v for k,v in zip(layer_names,layer_encodings)}\n",
    "    enc.update(pixel_enc)\n",
    "    encs.append(enc)\n",
    "    stim_sets.append(stim_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_to_depth = {\n",
    "    'dense_3':3,\n",
    "    'dense_2':2,\n",
    "    'dense_1':1,\n",
    "    'pixel':0,\n",
    "    'y_lat':4,'z_lat':4}\n",
    "\n",
    "assemblies = [raw_to_xr(encs[i],l_to_depth,stim_sets[i]) for i in np.arange(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def save_assembly(da,run_dir,fname,**kwargs):\n",
    "    da = da.reset_index(da.coords.dims)\n",
    "    da.attrs = OrderedDict()\n",
    "    with open(os.path.join(run_dir,fname), 'wb') as fp:\n",
    "        da.to_netcdf(fp,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for da,rd in zip(assemblies,runs):\n",
    "    \n",
    "    save_assembly(da,run_dir=rd,fname='dataset.nc',\n",
    "        format='NETCDF3_64BIT',\n",
    "#         engine=\n",
    "#         encoding=enc,\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stim_sets[0].dx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0f3cd8241457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcut_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mn_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-0f3cd8241457>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(n_idx)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z_lat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstim_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcut_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mn_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 5"
     ]
    }
   ],
   "source": [
    "exp = 2\n",
    "X = encs[exp]['z_lat']\n",
    "Y = stim_sets[exp].dx.values\n",
    "cut_r = lambda n_idx: np.abs(pearsonr(X[:,n_idx],Y)[0])\n",
    "\n",
    "zr = list(map(cut_r, np.arange(35)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = load_data_loader(configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = load_model(runs[2])\n",
    "l1_enc = Model(mod.input,mod.get_layer('dense_1').output)\n",
    "classifier = Model(mod.input,mod.layers[-1].output)\n",
    "classifier.compile(optimizer='nadam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "classifier.evaluate(x=DL.sx_test,y=DL.y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.concat(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variations(conf):\n",
    "    _,_,y_te = load_shifts(conf)\n",
    "    return y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(str(c.keys())+'\\n') for c in configs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = [load_shifts(c) for c in configs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxs = [s[0].astype(np.int32) for s in shifts]\n",
    "dys = [s[1].astype(np.int32) for s in shifts]\n",
    "y_tes = [s[2].astype(np.int8) for s in shifts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encs = [np.load(os.path.join(rd,'y_enc.npy')).astype(np.float32) for rd in runs]\n",
    "z_encs = [np.load(os.path.join(rd,'z_enc.npy')).astype(np.float32) for rd in runs]\n",
    "\n",
    "y_dim = [c['y_dim'] for c in configs]\n",
    "z_dim = [c['z_dim'] for c in configs]\n",
    "enc_dim = [c['enc_layers'] for c in configs]\n",
    "l3_dim = [ed[2] for ed in enc_dim]\n",
    "l2_dim = [ed[1] for ed in enc_dim]\n",
    "l1_dim = [ed[0] for ed in enc_dim]\n",
    "\n",
    "y_enc_dfs = [pd.DataFrame(data=ye,columns=['y_{}'.format(i) for i in np.arange(n_units)]) for ye,n_units in zip(y_encs,y_dim)]\n",
    "z_enc_dfs = [pd.DataFrame(data=ze,columns=['z_{}'.format(i) for i in np.arange(n_units)]) for ze,n_units in zip(z_encs,z_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_encs = [np.load(os.path.join(rd,'l3_enc.npy')).astype(np.float32) for rd in runs]\n",
    "l3_enc_dfs = [pd.DataFrame(data=l3e,columns=['l3_{}'.format(i) for i in np.arange(n_units[2])]) for l3e,n_units in zip(l3_encs,enc_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_encs = [np.load(os.path.join(rd,'l2_enc.npy')) for rd in runs]\n",
    "l2_enc_dfs = [pd.DataFrame(data=l2e,columns=['l2_{}'.format(i) for i in np.arange(n_units[1])]) for l2e,n_units in zip(l2_encs,enc_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_encs = [np.load(os.path.join(rd,'l1_enc.npy')) for rd in runs]\n",
    "l1_enc_dfs = [pd.DataFrame(data=l1e,columns=['l1_{}'.format(i) for i in np.arange(n_units[0])]) for l1e,n_units in zip(l1_encs,enc_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_fetch(configs,key):\n",
    "    return [conf[key] for conf in configs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_enc_dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import gaussian_kde\n",
    "# from scipy.stats import norm\n",
    "# Z = norm.ppf\n",
    "\n",
    "# def dicarlo_dprime(df,num_units=10,col='class'):\n",
    "#     uniq_cls = np.unique(df[col].values)\n",
    "#     o = []\n",
    "#     r = []\n",
    "#     F = []\n",
    "#     H = []\n",
    "#     d = []\n",
    "#     mask = list(~np.any(df.groupby(col).var().values[:,:num_units]==0,axis=0))\n",
    "# #     print(len(mask))\n",
    "#     cols = df.columns[mask+([True]*3)]\n",
    "# #     print(cols)\n",
    "#     mdf = df[cols]\n",
    "# #     print(mdf.head())\n",
    "#     for i in uniq_cls:\n",
    "#         oi = mdf[mdf[col]==i].values[:,:len(mdf.columns)-3]\n",
    "#         o_mu = oi.mean(axis=0)\n",
    "# #         print(oi.shape)\n",
    "#         o_kde = [gaussian_kde(oi[:,u]) for u in np.arange(oi.shape[1])]\n",
    "#         o.append(o_kde)\n",
    "\n",
    "#         ri = mdf[mdf[col]!=i].values[:,:len(mdf.columns)-3]\n",
    "#         r_kde = [gaussian_kde(ri[:,u]) for u in np.arange(ri.shape[1])]\n",
    "#         r.append(r_kde)\n",
    "#         F.append([k.integrate_box_1d(om,k.dataset.max()) for k,om in zip(r_kde,o_mu)])\n",
    "#         H.append([k.integrate_box_1d(low,om) for k,om,low in zip(o_kde,o_mu,oi.min(axis=0))])\n",
    "#         d.append(Z(H[-1])-Z(F[-1]))\n",
    "    \n",
    "# #     class_d_max = np.argmax(np.abs(d),axis=0)\n",
    "#     d = np.abs(d)\n",
    "#     class_d_max = np.argmax(d,axis=0)\n",
    "#     d_idxs = (class_d_max,np.arange(len(class_d_max)))\n",
    "#     d = d[d_idxs]\n",
    "#     return d,mask\n",
    "\n",
    "# def dicarlo_sel(df,num_units=10,col='class'):\n",
    "#     mu = df.groupby(col).mean().values[:,:num_units]\n",
    "#     var = df.groupby(col).var().values[:,:num_units]\n",
    "\n",
    "#     mu_max_idxs = np.argmax(mu,axis=0)\n",
    "#     mu_min_idxs = np.argmin(mu,axis=0)\n",
    "\n",
    "#     mu_max = np.array([mu[maxi,i] for i,maxi in zip(np.arange(len(mu_max_idxs)),mu_max_idxs)])\n",
    "#     mu_min = np.array([mu[maxi,i] for i,maxi in zip(np.arange(len(mu_min_idxs)),mu_min_idxs)])\n",
    "    \n",
    "#     var_b = np.array([var[maxi,i] for i,maxi in zip(np.arange(len(mu_max_idxs)),mu_max_idxs)])\n",
    "#     var_w = np.array([var[mini,i] for i,mini in zip(np.arange(len(mu_min_idxs)),mu_min_idxs)])\n",
    "# #     neg_max = [(allv-mu_max)/9.0 for allv,mu_max in zip(all_vals.sum(axis=1),mu_max)]\n",
    "    \n",
    "#     sel = [(mu_b-mu_w)/np.sqrt((vb+vw)/2) for mu_b,mu_w,vb,vw in zip(mu_max,mu_min,var_b,var_w)]\n",
    "#     return sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(o[9][3].dataset,hist=False)\n",
    "# g = sns.distplot(NS[9][3].dataset,hist=False)\n",
    "# g.legend(['stim','no-stim'])\n",
    "\n",
    "obj_names = [\n",
    "    \"T-shirt\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Dress Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_xa = []\n",
    "for y_te,dx,dy,df,zdf,l3df,l2df,l1df,conf in zip(y_tes,dxs,dys,y_enc_dfs,z_enc_dfs,l3_enc_dfs,l2_enc_dfs,l1_enc_dfs,configs):\n",
    "    df['class'] = y_te\n",
    "    zdf['class'] = y_te\n",
    "    l3df['class'] = y_te\n",
    "    l2df['class'] = y_te\n",
    "    l1df['class'] = y_te\n",
    "    \n",
    "    df['dx'] = dx\n",
    "    zdf['dx'] = dx\n",
    "    l3df['dx'] = dx\n",
    "    l2df['dx'] = dx\n",
    "    l1df['dx'] = dx\n",
    "    \n",
    "    df['dy'] = dy\n",
    "    zdf['dy'] = dy\n",
    "    l3df['dy'] = dy\n",
    "    l2df['dy'] = dy\n",
    "    l1df['dy'] = dy\n",
    "    ydf = df\n",
    "    \n",
    "    layer_names = ['y','z','l1','l2','l3']\n",
    "    depths = [4,4,1,2,3]\n",
    "    names = ['y','z',None,None,None]\n",
    "    layer_das = [make_xr(d,depth,region=n) for d,depth,n in zip([ydf,zdf,l1df,l2df,l3df],depths,names)]\n",
    "    ably = xarray.concat(layer_das,dim='neuroid')\n",
    "    stim = ably.presentation.to_dataframe().reset_index()\n",
    "    stim['image_id'] = [hashlib.md5(json.dumps(list(p),sort_keys=True).encode('utf-8')).digest().hex() for p in stim['presentation'].values]\n",
    "    # Add attributes brain-score expects\n",
    "    ably = ably.assign_attrs({\n",
    "#         'hyperparameters': pd.DataFrame([conf]),\n",
    "        'stimulus_set':stim.drop(columns=['presentation'])\n",
    "    })\n",
    "    exp_xa.append(ably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = exp_xa[1]\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_conds = [\n",
    "    'Only XEnt',\n",
    "    'Only Recon',\n",
    "    'Both',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.Dataset({k:e for k,e in zip(exp_conds,exp_xa)})\n",
    "# da = make_xr(zdf)\n",
    "# da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.join(runs[2],'recon.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.coords.dims.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = exp_xa[0]\n",
    "da = da.reset_index(da.coords.dims)\n",
    "# da = da.assign_attrs(stimulus_set=da.attrs['stimulus_set'].values)\n",
    "# da.attrs = OrderedDict()\n",
    "print(da)\n",
    "fname='dataset.nc'\n",
    "with open(os.path.join(runs[0],fname), 'wb') as fp:\n",
    "    da.to_netcdf(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.attrs['stimulus_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "for da,cond,rd in zip(exp_xa,exp_conds,runs):\n",
    "    \n",
    "    save_assembly(da,run_dir=rd,fname='dataset.nc',\n",
    "        format='NETCDF3_64BIT',\n",
    "#         engine=\n",
    "#         encoding=enc,\n",
    "       )\n",
    "\n",
    "# ds.to_netcdf(encoding=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xa_calc_sel(xas,confs,props=['class','dx','dy']):\n",
    "        ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sel(xa,confs,num_units,props):\n",
    "    exp_conds = {\n",
    "        'xent' : conf_fetch(confs,'xent'),\n",
    "        'recon': conf_fetch(confs,'recon'),\n",
    "        'label_corruption' :conf_fetch(confs,'label_corruption'),\n",
    "        'ecc_max': conf_fetch(confs,'ecc_max')\n",
    "    }\n",
    "    exp_conds = pd.DataFrame.from_records(exp_conds)\n",
    "    exp_conds_l = [{k:v[i] for k,v in exp_conds.iteritems()} for i in np.arange(len(confs))]\n",
    "    \n",
    "    affinities = []\n",
    "    for p in props:\n",
    "        if p in ['class']:\n",
    "            dprimes = [dicarlo_dprime(df,num_units=num_units,col=p) for df in dfs]\n",
    "            prop_sel = [d[0] for d in dprimes]\n",
    "            units = [np.where(np.array(d[1])==True)[0] for d in dprimes]\n",
    "        elif p in ['dx','dy']:\n",
    "            props = [df[p].values for df in dfs]\n",
    "#             print([p.shape for p in props])\n",
    "            activations = [df.values[:,:num_units] for df in dfs]\n",
    "            prop_sel = [dicarlo_r(a,p) for p,a in zip(props,activations)]\n",
    "            units = [np.arange(num_units)]*len(confs)\n",
    "        \n",
    "        recs = [{'unit':u,'affinity':a,'property':p} for i,u,a in zip(np.arange(len(dfs)),units,prop_sel)]\n",
    "        for r,cond in zip(recs,exp_conds_l):\n",
    "            r.update(cond)\n",
    "            \n",
    "        prop_dfs = [pd.DataFrame.from_records(r) for r in recs]\n",
    "#         prop_sel_df = prop_sel_df.join(exp_conds)\n",
    "#         prop_sel_df['property']=p\n",
    "\n",
    "        affinities.append(pd.concat(prop_dfs))\n",
    "        \n",
    "    return pd.concat(affinities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_szs = list(zip(y_dim,z_dim,l3_dim,l2_dim,l1_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aff = calc_sel(l3_enc_dfs,confs=configs,num_units=500,props=['class','dx'])\n",
    "# aff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = []\n",
    "layer_depths = [4,4,3,2,1]\n",
    "layer_names = ['y','z','l3','l2','l1']\n",
    "for dfs,n_u in zip([y_enc_dfs,z_enc_dfs,l3_enc_dfs,l2_enc_dfs,l1_enc_dfs],layer_szs[0],):\n",
    "    print('n_u: ',n_u)\n",
    "    pop_aff = calc_sel(dfs,confs=configs,num_units=n_u,props=['class','dx','dy'])\n",
    "    pops.append(pop_aff)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,l_depth,l_name in zip(pops,layer_depths,layer_names):\n",
    "    print('lname: ',l_name)\n",
    "#     ldf = df.melt(id_vars=['recon','xent','property','ecc_max','label_corruption'],var_name='unit',value_name='affinity')\n",
    "    df['layer_depth'] = l_depth\n",
    "    df['layer_name'] = l_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat(pops)\n",
    "queries = [\n",
    "        'label_corruption == 0.0 & xent == 15 & recon == 0 & ecc_max == 0.8',\n",
    "        'label_corruption == 0.0 & xent == 0 & recon == 25 & ecc_max == 0.8',\n",
    "        'label_corruption == 0.0 & xent == 15 & recon == 25 & ecc_max == 0.8',\n",
    "              ]\n",
    "titles = [\n",
    "    'Only XEnt',\n",
    "    'Only Recon',\n",
    "    'Both'\n",
    "]\n",
    "subs = []\n",
    "for cond,q in zip(titles,queries): \n",
    "    sub = all_df.query(q)\n",
    "    sub['condition'] = [cond]*len(sub)\n",
    "    subs.append(sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat(subs)\n",
    "all_c_df = all_df.query('property == \"class\"')\n",
    "all_x_df = all_df.query('property == \"dx\"')\n",
    "all_y_df = all_df.query('property == \"dy\"')\n",
    "all_pos_df = all_df.query('property == \"dx\" | property == \"dy\"').pivot_table(index=['recon','xent','layer_name','unit'],columns=['property'],values='affinity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_df.groupby(['recon','xent','layer_name']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_affinity_profile(dset,prop='class',plot_func=sns.barplot,legend_panel=0,**kwargs):\n",
    "    fig,axs = plt.subplots(3,1,figsize=(4,12),sharey=True,sharex=True)\n",
    "    queries = [\n",
    "        'label_corruption == 0.0 & xent == 15 & recon == 0 & ecc_max == 0.8',\n",
    "        'label_corruption == 0.0 & xent == 0 & recon == 25 & ecc_max == 0.8',\n",
    "        'label_corruption == 0.0 & xent == 15 & recon == 25 & ecc_max == 0.8',\n",
    "              ]\n",
    "    titles = [\n",
    "        'Only XEnt',\n",
    "        'Only Recon',\n",
    "        'Both'\n",
    "    ]\n",
    "#     sns.set_context('talk')\n",
    "    for ax,q,title in zip(axs,queries,titles): \n",
    "        x_order = ['l1','l2','l3']\n",
    "#         if rowi == 0:\n",
    "#             x_order.append('y')\n",
    "#         else:\n",
    "#             x_order.append('z')\n",
    "            \n",
    "        g = plot_func(y='affinity',x='layer_depth',\n",
    "                  hue='layer_name',\n",
    "#                       data=dset.query(q).query('layer_name != \"{}\"'.format(exclude)),\n",
    "                  data=dset.query(q),ax=ax,**kwargs)\n",
    "        if title != titles[legend_panel]:\n",
    "            ax.get_legend().remove()\n",
    "        \n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('{} selectivity'.format(prop))\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    axs[-1].set_xlabel('Model Layers')\n",
    "    axs[0].legend(loc='upper center', bbox_to_anchor=(1.5, 1.0), shadow=True, ncol=1)\n",
    "#         axs[rowi,0].set_ylabel(row_lab.format(prop))\n",
    "#         axs[rowi,1].set_ylabel('')\n",
    "#         axs[rowi,2].set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_affinity_profile_by_layer(dset,prop='class',plot_func=sns.barplot):\n",
    "    fig,axs = plt.subplots(2,5,figsize=(20,10),sharey=True)\n",
    "#     queries = [\n",
    "#         'label_corruption == 0.0 & xent == 15 & recon == 0 & ecc_max == 0.8',\n",
    "#         'label_corruption == 0.0 & xent == 0 & recon == 25 & ecc_max == 0.8',\n",
    "#         'label_corruption == 0.0 & xent == 15 & recon == 25 & ecc_max == 0.8',\n",
    "#               ]\n",
    "#     queries = ['']\n",
    "    titles=['Layer {}'.format(l) for l in np.arange(3)+1]\n",
    "    titles.append('Layer y+z')\n",
    "    for rowi,row_lab,exclude in zip(np.arange(2),['ventral {} affinity (y)','dorsal {} affinity (z)'],[\"z\",\"y\"]):\n",
    "        for ax,layer_n,title in zip(axs[rowi],[1,2,3,4],titles): \n",
    "#             x_order = [1,2,3,'l4']\n",
    "            if rowi == 0:\n",
    "                pass\n",
    "#                 x_order.append('y')\n",
    "            else:\n",
    "                pass\n",
    "#                 x_order.append('z')\n",
    "            plot_func(y='affinity',x='condition',\n",
    "                           hue='condition',\n",
    "                           data=dset.query('layer_depth == {}'.format(layer_n)),ax=ax)\n",
    "            ax.set_title(title)\n",
    "        axs[rowi,0].set_ylabel(row_lab.format(prop))\n",
    "        axs[rowi,1].set_ylabel('')\n",
    "        axs[rowi,2].set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.violinplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "stream_affinity_profile(all_c_df,prop='class',plot_func=sns.boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dx and Dy Selectivity Results by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_affinity_profile(all_x_df,prop='dx',plot_func=sns.boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_affinity_profile(all_y_df,prop='dy',plot_func=sns.boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaled_class = scaler.fit_transform(all_df.query('property == \"class\"').dropna()['affinity'].values.reshape(-1,1))\n",
    "\n",
    "norm_class = all_df.query('property == \"class\"').dropna()[['recon','xent','ecc_max','label_corruption','unit','layer_depth','layer_name']]\n",
    "norm_class['property']='norm_class'\n",
    "norm_class['affinity']=np.squeeze(rescaled_class)\n",
    "# norm_class = pd.DataFrame.from_records({'affinity': np.squeeze(rescaled_class),'property':'norm_class'})\n",
    "# norm_class.index = all_df.query('property == \"class\"').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_ = all_df.append(norm_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df_\n",
    "ydf = all_df.query('layer_name == \"y\"').reset_index()\n",
    "ydf = pd.pivot_table(ydf,index=['xent','recon','unit'],columns='property',values='affinity')\n",
    "ydf['layer_name']= 'y'\n",
    "\n",
    "zdf = all_df.query('layer_name == \"z\"').reset_index()\n",
    "zdf = pd.pivot_table(zdf,index=['xent','recon','unit'],columns='property',values='affinity')\n",
    "zdf['layer_name']= 'z'\n",
    "\n",
    "l4df = pd.concat([ydf,zdf])\n",
    "\n",
    "l3df = all_df.query('layer_name == \"l3\"').reset_index()\n",
    "l3df = pd.pivot_table(l3df,index=['xent','recon','unit'],columns='property',values='affinity')\n",
    "l3df['layer_name']= 'l3'\n",
    "\n",
    "\n",
    "l2df = all_df.query('layer_name == \"l2\"').reset_index()\n",
    "l2df = pd.pivot_table(l2df,index=['xent','recon','unit'],columns='property',values='affinity')\n",
    "l2df['layer_name']= 'l2'\n",
    "\n",
    "\n",
    "l1df = all_df.query('layer_name == \"l1\"').reset_index()\n",
    "l1df = pd.pivot_table(l1df,index=['xent','recon','unit'],columns='property',values='affinity')\n",
    "l1df['layer_name']= 'l1'\n",
    "\n",
    "# ydf['dx_selectivity'] = y_x_sel_df['dx_selectivity']\n",
    "# zdf = z_c_sel_df\n",
    "# zdf['dx_selectivity'] = z_x_sel_df['dx_selectivity']\n",
    "# l3df = l3_c_sel_df\n",
    "# l3df['dx_selectivity'] = l3_x_sel_df['dx_selectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_scatter(x='dx',y='dy',dsets=[l1df,l2df,l3df,l4df,],plot_func=sns.scatterplot,**plot_kw):\n",
    "    filts =[\n",
    "        'recon == 0 & xent == 15',\n",
    "        'recon == 25 & xent == 0',\n",
    "        'recon == 25 & xent == 15',\n",
    "        \n",
    "    ]\n",
    "    fig,axs = plt.subplots(len(dsets),3,figsize=(2.5*3,2.5*len(dsets),),\n",
    "    #                        subplot_kw={'xlim':(-.1,1),'ylim':(-0.1,1)},\n",
    "                           sharex=True,sharey=True)\n",
    "    objective_conds = ['Only Xent','Only Recon','Xent+Recon']\n",
    "    for filt,coli,obj in zip(filts,np.arange(3),objective_conds):\n",
    "        for ax,df,rowi in zip(axs[:,coli],dsets,np.arange(len(dsets))):\n",
    "            plot_func(x=x,y=y,data=df.query(filt),ax=ax,**plot_kw)\n",
    "            if coli == 0:\n",
    "                ax.set_ylabel('Layer {}'.format(rowi+1))\n",
    "            \n",
    "        axs[0,coli].set_title(obj)\n",
    "# sns.scatterplot(x=xvar,y=yvar,data=ydf.query(filt),ax=axs[1])\n",
    "# sns.scatterplot(x=xvar,y=yvar,data=zdf.query(filt),ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_kde(x='dx',y='dy',dsets=[l1df,l2df,l3df,l4df,],clip=(0,1),rug_plot=False,**plot_kw):\n",
    "    filts =[\n",
    "        'recon == 0 & xent == 15',\n",
    "        'recon == 25 & xent == 0',\n",
    "        'recon == 25 & xent == 15',\n",
    "        \n",
    "    ]\n",
    "    fig,axs = plt.subplots(3,len(dsets),figsize=(3*len(dsets),3*3,),\n",
    "                           subplot_kw={'xlim':(0,1),'ylim':(0,1)},\n",
    "                           sharex=True,sharey='row')\n",
    "    objective_conds = ['Only Xent','Only Recon','Xent+Recon']\n",
    "    layer_names = ['1','2','3','y+z',]\n",
    "    for coli,df,layer_lab in zip(np.arange(len(dsets)),dsets,layer_names):\n",
    "        for filt,ax,obj in zip(filts,axs[:,coli],objective_conds):\n",
    "            fdf = df.query(filt).dropna()\n",
    "            if coli != len(dsets)-1:\n",
    "                sns.kdeplot(data=fdf[x].values,data2=fdf[y].values,ax=ax,clip=clip,**plot_kw)\n",
    "            else:\n",
    "                ax.scatter(fdf.query('layer_name == \"y\"')[x].values,fdf.query('layer_name == \"y\"')[y].values,marker='.',**plot_kw)\n",
    "                ax.scatter(fdf.query('layer_name == \"z\"')[x].values,fdf.query('layer_name == \"z\"')[y].values,marker='.',c='orange',**plot_kw)\n",
    "\n",
    "#                 ax.set_ylim(0,)\n",
    "                \n",
    "            if layer_lab == layer_names[-1] and rug_plot:\n",
    "                sns.rugplot(fdf.query('layer_name == \"y\"')[x].values,ax=ax)\n",
    "                sns.rugplot(fdf.query('layer_name == \"z\"')[x].values,ax=ax,color=\"g\")\n",
    "                sns.rugplot(fdf.query('layer_name == \"y\"')[y].values,ax=ax,vertical=True)\n",
    "                sns.rugplot(fdf.query('layer_name == \"z\"')[y].values,ax=ax,color=\"g\",vertical=True)\n",
    "#                 ax.scatter(fdf[x].values,fdf[y].values,c=\"w\",marker=\"+\",linewidths=1)\n",
    "            if coli == 0:\n",
    "                ax.set_ylabel('{}'.format(obj))\n",
    "            \n",
    "            if obj == objective_conds[-1]:\n",
    "                ax.set_xlabel(x)\n",
    "            \n",
    "        axs[0,coli].set_title('Layer {}'.format(layer_lab))\n",
    "#         axs[0,coli].set_ylabel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l3df.query(f).dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.kdeplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_kde(\n",
    "#     clip=(-0.1,0.75),\n",
    "#     shade=True,shade_lowest=False,\n",
    ")\n",
    "# plt.xlim(0,0.6)\n",
    "# plt.ylim(0,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_kde(x='norm_class',y='dx',\n",
    "#         clip=(-0.1,1),\n",
    "#         shade=True,shade_lowest=False\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.kdeplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = 'recon == 25 & xent == 15'\n",
    "xvar = 'norm_class'\n",
    "yvar = 'dx'\n",
    "\n",
    "g = sns.JointGrid(x=xvar,y=yvar,data=l4df.query(filt),space=0.5,\n",
    "#                   xlim=(-0.05,0.5),\n",
    "#                   ylim=(-0.05,0.5)\n",
    "                 )\n",
    "g = g.plot_joint(sns.kdeplot,\n",
    "                 shade=True,shade_lowest=False,\n",
    "                )\n",
    "# g.ax_joint.set_xlim(-0.05,0.5)\n",
    "# g.ax_joint.set_ylim(-0.05,0.9)\n",
    "# g = g.plot_marginals(sns.kdeplot)\n",
    "sns.kdeplot(l4df.query('{} & layer_name == \"y\"'.format(filt))[xvar],ax=g.ax_marg_x,legend=None)\n",
    "sns.kdeplot(l4df.query('{} & layer_name == \"z\"'.format(filt))[xvar],ax=g.ax_marg_x,c=\"orange\",legend=None)\n",
    "sns.kdeplot(l4df.query('{} & layer_name == \"y\"'.format(filt))[yvar],ax=g.ax_marg_y,legend=None,vertical=True)\n",
    "sns.kdeplot(l4df.query('{} & layer_name == \"z\"'.format(filt))[yvar],ax=g.ax_marg_y,c=\"orange\",vertical=True)\n",
    "g.ax_marg_y.legend(['y','z'],loc='upper center', bbox_to_anchor=(1.7, 1.1), shadow=True, ncol=1)\n",
    "\n",
    "g.ax_marg_x.set_title('Both')\n",
    "# sns.rugplot(l4df.query('{} & layer_name == \"y\"'.format(filt))[yvar],ax=g.ax_joint,vertical=True)\n",
    "# sns.rugplot(l4df.query('{} & layer_name == \"z\"'.format(filt))[yvar],ax=g.ax_joint,c=\"g\",vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_kde(y='dy',x='norm_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scatter(x='dx',y='norm_class',hue='layer_name',legend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scatter(xvar='dx',yvar='norm_class',hue='layer_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scatter(xvar='dy',yvar='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(15,5),\n",
    "#                        subplot_kw={'xlim':(0,3),'ylim':(0,3)},\n",
    "                       sharex=True,sharey=True)\n",
    "\n",
    "filt = 'recon == 25 & xent == 0'\n",
    "xvar = 'dx'\n",
    "yvar = 'dy'\n",
    "sns.scatterplot(x=xvar,y=yvar,data=l3df.query(filt),ax=axs[0])\n",
    "sns.scatterplot(x=xvar,y=yvar,data=ydf.query(filt),ax=axs[1])\n",
    "sns.scatterplot(x=xvar,y=yvar,data=zdf.query(filt),ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(15,5),\n",
    "#                        subplot_kw={'xlim':(0,3),'ylim':(0,3)},\n",
    "                       sharex=True,sharey=True)\n",
    "\n",
    "filt = 'recon == 25 & xent == 15'\n",
    "xvar = 'dx'\n",
    "yvar = 'dy'\n",
    "sns.scatterplot(x=xvar,y=yvar,data=l3df.query(filt),ax=axs[0])\n",
    "sns.scatterplot(x=xvar,y=yvar,data=ydf.query(filt),ax=axs[1])\n",
    "sns.scatterplot(x=xvar,y=yvar,data=zdf.query(filt),ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydf[['recon','xent','unit']]\n",
    "ydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(15,5),\n",
    "#                        subplot_kw={'xlim':(0,3),'ylim':(0,3)},\n",
    "                       sharex=True,sharey=True)\n",
    "sns.scatterplot(x='dx',y='class',data=l3df.reset_index().query('recon == 0 & xent == 15'),ax=axs[0])\n",
    "sns.scatterplot(x='dx',y='class',data=ydf.query('recon == 0 & xent == 15'),ax=axs[1])\n",
    "sns.scatterplot(x='dx',y='class',data=zdf.query('recon == 0 & xent == 15'),ax=axs[2])\n",
    "\n",
    "# plt.xlim(0.0,3)\n",
    "# plt.ylim(0.0,3)\n",
    "axs[0].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[0].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[0].set_title('l3 latent space')\n",
    "axs[1].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[1].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[1].set_title('y latent space')\n",
    "axs[2].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[2].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[2].set_title('z latent space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(15,5),\n",
    "#                        subplot_kw={'xlim':(0,3),'ylim':(0,3)},\n",
    "                       sharex=True,sharey=True)\n",
    "sns.scatterplot(x='dx',y='class',data=l3df.query('recon == 25 & xent == 15'),ax=axs[0])\n",
    "sns.scatterplot(x='dx',y='class',data=ydf.query('recon == 25 & xent == 15'),ax=axs[1])\n",
    "sns.scatterplot(x='dx',y='class',data=zdf.query('recon == 25 & xent == 15'),ax=axs[2])\n",
    "\n",
    "axs[0].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[0].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[0].set_title('l3 latent space')\n",
    "axs[1].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[1].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[1].set_title('y latent space')\n",
    "axs[2].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[2].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[2].set_title('z latent space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(15,5),\n",
    "#                        subplot_kw={'xlim':(0,3),'ylim':(0,3)},\n",
    "                       sharex=True,sharey=True)\n",
    "sns.scatterplot(x='dx',y='class',data=l3df.query('recon == 25 & xent == 0'),ax=axs[0])\n",
    "sns.scatterplot(x='dx',y='class',data=ydf.query('recon == 25 & xent == 0'),ax=axs[1])\n",
    "sns.scatterplot(x='dx',y='class',data=zdf.query('recon == 25 & xent == 0'),ax=axs[2])\n",
    "\n",
    "# plt.xlim(0.0,3)\n",
    "# plt.ylim(0.0,3)\n",
    "axs[0].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[0].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[0].set_title('l3 latent space')\n",
    "axs[1].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[1].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[1].set_title('y latent space')\n",
    "axs[2].hlines(y=0.66,xmin=0,xmax=2,linestyles='dashed')\n",
    "axs[2].vlines(x=0.66,ymin=0,ymax=2,linestyles='dashed')\n",
    "axs[2].set_title('z latent space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='class_selectivity',x='xent',hue='ecc_max',data=y_sel_df.query('label_corruption == 0.0 & recon == 25'),palette='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(y='class_selectivity',x='ecc_max',hue='xent',data=l2_sel_df.query('label_corruption == 0.0'),palette='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='class_selectivity',x='recon',hue='ecc_max',data=y_sel_df.query('label_corruption == 0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='class_selectivity',x='recon',hue='ecc_max',data=z_sel_df.query('label_corruption == 0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='class_selectivity',x='ecc_max',hue='recon',data=z_sel_df.query('label_corruption == 0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_fetch(configs,'xent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-GPU (Python3.5.2)",
   "language": "python",
   "name": "py3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
